







sh buildwebots.sh   ä½¿ç”¨è¿™ä¸ªè°ƒç”¨è™šæ‹Ÿç¯å¢ƒ
sh buildrobot.sh    è°ƒç”¨ä¸Šæœºç¯å¢ƒ



### conda å‘½ä»¤ 

conda deactivate
conda env remove -n TianG

conda create -n TianG python=3.10 -y
conda activate TianG
pip install -U pip



g++ -std=c++17 -Wall -Wextra -Wpedantic -g src/hello.cpp -o build/hello ä¸¥æ ¼è°ƒè¯•C++


tree -L 3  æŸ¥çœ‹3çº§ç›®å½•



### ros2 å‘½ä»¤è¡Œ

åˆ‡æ¢ros

source /opt/ros/noetic/setup.bash 

source /opt/ros/humble/setup.bash


colcon build ä¼šå°†æ„å»ºç»“æœå®‰è£…åˆ°ä¸€ä¸ªç›®å½•ä¸­ï¼Œé»˜è®¤æ˜¯åœ¨ install/ ç›®å½•ä¸‹ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æºå®‰è£…è·¯å¾„æ¥ä½¿å¾—å·¥ä½œç©ºé—´ä¸­çš„æ‰€æœ‰å¯æ‰§è¡Œæ–‡ä»¶ã€åº“æ–‡ä»¶å’Œ Python åŒ…å¯ç”¨ï¼š 

source install/setup.bash

ros2 node list æŸ¥çœ‹å½“å‰çš„node
ros2 node info /turtlesim æŸ¥çœ‹è¿™ä¸ªnodeä¸‹çš„è®¢é˜… æœåŠ¡

ros2 topic list æŸ¥çœ‹å½“å‰çš„topicæœ‰å“ªäº›
ros2 topic echo /turtle/pose  ä¸ºä¾‹ æ‰“å°å‡ºå½“å‰çš„è¯é¢˜é‡Œé¢çš„å†…å®¹

ros2 bag record /turtle1/cmd_vel ï¼ˆè¯é¢˜ï¼‰ å¼€å§‹è®°å½•æŸä¸ªè¯é¢˜çš„æ•°æ®
ros2 bag play rosbag2_2025_12_30-15_36_29/(æ–‡ä»¶)  æ’­æ”¾è®°å½•çš„æ•°æ®

1d27ecf38583de0c0dc337bd0805f6ca

ros2 pkg create --build-type ament_cmake learning_package_c  åˆ›å»ºC++åŠŸèƒ½åŒ…
ros2 pkg create --build-type ament_python learning_package_py  åˆ›å»ºpythonåŠŸèƒ½åŒ…



### C++å‘½ä»¤







### ä»£ç å¤‡å¿˜å½•









# å¤©å·¥ä»£ç 

## ä»£ç è°ƒç”¨å‘½ä»¤

### Tensorboard

**è°ƒå‡ºæœ€è¿‘çš„checkpointå’Œ tensorboard**

```bash
latest_run=$(ls -dt logs/*/* | head -n 1)
echo "LATEST RUN: $latest_run"

echo "TensorBoard events:"
find "$latest_run" -type f -name "events.out.tfevents.*"

echo "Checkpoints:"
find "$latest_run" -type f \( -name "*.pt" -o -name "*.pth" \)
```

**æŸ¥çœ‹TensorBoard**

æ–¹å¼ Aï¼šæŒ‡å‘ experimentï¼ˆæ¨èï¼Œä¾¿äºå¯¹æ¯”å¤šæ¬¡ runï¼‰

```
tensorboard --logdir logs/walk --bind_all --port 6006
```

æ–¹å¼ Bï¼šåªçœ‹è¿™ä¸€æ¬¡ run

```
tensorboard --logdir logs/walk/2025-12-30_19-36-27 --bind_all --port 6006
```

ç„¶åæœ¬åœ°æ‰“å¼€ `http://localhost:6006`



### Train

```
python legged_lab/scripts/train.py --task=walk --headless --logger=tensorboard --num_envs=4096
python legged_lab/scripts/train.py --task=run --headless --logger=tensorboard --num_envs=4096
```

### Play

```
python legged_lab/scripts/play.py --task=walk --num_envs=1
```

ä¿®æ”¹ /home/mig/Documents/TienKung-Lab/legged_lab/envs/tienkung/walk_cfg.py   æ–‡ä»¶ä¸­çš„ï¼šæ¥ä¿è¯ä½ ç°åœ¨playçš„åˆ°åº•æ˜¯å“ªä¸€ä¸ªpt

```python
	 resume = False

â€‹    \#load_run = ".*" #2026-01-05_19-55-14

â€‹    load_run = "2026-01-06_16-13-51" #2026-01-05_19-55-14

â€‹    \#load_checkpoint = "model_.*.pt"

â€‹    load_checkpoint = "model_14500.pt"

```



### Sim2Sim(MuJoCo)

å› ä¸ºsiméœ€è¦çš„ptæ–‡ä»¶å‡ºç°ä¸èƒ½ç”¨çš„æƒ…å†µï¼Œæˆ‘ä»trainå¯¼å‡ºçš„åŸå§‹ptä¸­åˆå¯¼äº†ä¸€ä¸‹ã€‚è°ƒç”¨ä¸‹è¿°ä»£ç ï¼š

```bash
  python export_jit_policy_from_ckpt.py \

â€‹    --ckpt logs/walk/2026-01-06_16-13-51/model_15000.pt \

â€‹    --out  Exported_policy/walk_from_ckpt_15000.pt \

â€‹    --obs-dim 750 \

â€‹    --act-dim 20


```

è°ƒç”¨ä»¿çœŸæ¨¡å—ï¼šï¼ˆå¦‚æœåŸå§‹çš„walk.ptæ–‡ä»¶å¯ä»¥ç”¨ï¼‰ï¼š

```bash
python legged_lab/scripts/sim2sim.py --task walk --policy Exported_policy/walk.pt --duration 100
```

å¦‚æœä¸èƒ½ç”¨ï¼š

```bash
python legged_lab/scripts/sim2sim.py --task walk --policy Exported_policy/walk_from_ckpt_15000.pt --duration 100
```





## å·¥ç¨‹æ¡†æ¶

æŠŠå®ƒæƒ³æˆå››å±‚ï¼š

**Layer Aï¼šä»¿çœŸåº•åº§ï¼ˆIsaac Sim / Isaac Labï¼‰**

- è´Ÿè´£ï¼šç‰©ç†ä»¿çœŸã€GPU PhysXã€èµ„äº§åŠ è½½ï¼ˆUSD articulationï¼‰ã€ä¼ æ„Ÿå™¨ã€å¹¶è¡Œç¯å¢ƒå¤åˆ¶

**Layer Bï¼šä»»åŠ¡ä¸ MDPï¼ˆlegged_labï¼‰**

- è´Ÿè´£ï¼šå®šä¹‰ä»»åŠ¡ï¼ˆwalkï¼‰ã€å®šä¹‰è§‚æµ‹ã€åŠ¨ä½œæ˜ å°„ã€å¥–åŠ±ï¼ˆmdp.*ï¼‰ã€ç»ˆæ­¢æ¡ä»¶ã€åŸŸéšæœºåŒ–ã€å‘½ä»¤ç”Ÿæˆå™¨
- ä½ å‘çš„ `LiteRewardCfg`ã€`TienKungWalkFlatEnvCfg` å°±åœ¨è¿™ä¸€å±‚

**Layer Cï¼šå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆrsl_rl / isaaclab_rl.rsl_rlï¼‰**

- è´Ÿè´£ï¼šPPOã€AMP PPOã€RNDã€å¯¹ç§°æ€§çº¦æŸã€rollout storageã€GAEã€ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨

**Layer Dï¼šè„šæœ¬ä¸æ³¨å†Œï¼ˆscripts + task_registryï¼‰**

- è´Ÿè´£ï¼šæŠŠ cfg + env class æ³¨å†Œåˆ° registryï¼Œæä¾› train.py è¿™æ ·çš„å…¥å£æŠŠæ‰€æœ‰ç»„ä»¶æ‹¼èµ·æ¥



1. `legged_lab/scripts/train.py`ï¼ˆä½ å·²ç»åœ¨çœ‹ï¼‰
2. `legged_lab/utils/task_registry.py`ï¼ˆæœ€å…³é”®ï¼šwalk å¦‚ä½•æ˜ å°„åˆ° cfg+classï¼‰
3. `legged_lab/envs/<tienkung>/...` é‡Œ walk å¯¹åº”çš„ env æ–‡ä»¶ï¼ˆçœŸæ­£çš„ `env_class`ï¼‰
4. `legged_lab/mdp/__init__.py` ä¸ reward å‡½æ•°æ–‡ä»¶ï¼ˆä½ å·²ç»è´´è¿‡ï¼‰
5. `rsl_rl/runners/amp_on_policy_runner.py`ï¼ˆlearn å¾ªç¯ä¸ AMP å¦‚ä½•æ··åˆ rewardï¼‰
6. `rsl_rl/algorithms/amp_ppo.py`ï¼ˆAMPPPO çš„ loss å…¬å¼ä¸è®­ç»ƒæ­¥éª¤ï¼‰

è¿™æ ·ä½ ä¼šéå¸¸å¿«åœ°æŠŠâ€œå…¥å£ â†’ æ³¨å†Œ â†’ ç¯å¢ƒ â†’ reward â†’ runner â†’ algorithmâ€ä¸²èµ·æ¥ã€‚

### ç³»ç»Ÿé—­ç¯

**ä½ è·‘ `python train.py --task=walk ...` åï¼Œç³»ç»Ÿå‘ç”Ÿçš„ 3 ä¸ªé—­ç¯**

**é—­ç¯ 1ï¼šæ§åˆ¶é—­ç¯ï¼ˆPolicy â†’ Environment â†’ Next Obsï¼‰**

1. policy æ ¹æ® obs + command è¾“å‡º action
2. env.step(action) åœ¨ Isaac Sim é‡Œæ¨è¿›è‹¥å¹²ç‰©ç†æ­¥ï¼ˆdecimationï¼‰
3. env è¿”å› next_obsã€rewardã€doneã€infoï¼ˆå¹¶è¡Œ N ä¸ª envï¼‰

**é—­ç¯ 2ï¼šå­¦ä¹ é—­ç¯ï¼ˆRollout â†’ GAE â†’ PPO æ›´æ–°ï¼‰**

1. é‡‡æ ·ä¸€æ®µ rolloutï¼ˆnum_steps_per_envï¼‰
2. ç”¨ critic ä¼°è®¡ valueï¼Œç®—ä¼˜åŠ¿ A_tï¼ˆGAEï¼‰ä¸ returns
3. PPO å¤š epochã€å¤š mini-batch æ›´æ–° actor/critic

**é—­ç¯ 3ï¼šAMP é—­ç¯ï¼ˆExpert vs Policy â†’ Discriminator â†’ AMP rewardï¼‰**

1. ä» expert motion æ–‡ä»¶é‡‡æ ·â€œä¸“å®¶ç‰‡æ®µâ€ï¼ˆexpert transitionsï¼‰
2. ä»å½“å‰ policy rollout æŠ½â€œç­–ç•¥ç‰‡æ®µâ€ï¼ˆpolicy transitionsï¼‰
3. è®­ç»ƒåˆ¤åˆ«å™¨åŒºåˆ† expert/policy
4. ç”¨åˆ¤åˆ«å™¨è¾“å‡ºæ„é€  AMP rewardï¼Œæ··åˆ°æ€» reward é‡Œåå‘å¡‘å½¢ policy

------

#### 1) `legged_lab/scripts/train.py`

å®ƒä¸æ‡‚â€œèµ°è·¯ç»†èŠ‚â€ï¼Œåªåšæ‹¼è£…ä¸å¯åŠ¨ï¼š

- è§£æå‚æ•°ï¼š`--task=walk --num_envs=64 --headless --logger=tensorboard`
- å¯åŠ¨ Isaac Simï¼š`AppLauncher`
- è§¦å‘ä»»åŠ¡æ³¨å†Œï¼š`from legged_lab.envs import *`
- ä» registry æ‹¿ä¸‰æ ·ä¸œè¥¿ï¼š`env_cfg, agent_cfg, env_class`
- ç”¨ cfg åˆ›å»º envï¼Œç”¨ agent_cfg åˆ›å»º runner
- `runner.learn()` è¿›å…¥è®­ç»ƒä¸»å¾ªç¯

> train.py æ˜¯â€œæ€»æ§å¼€å…³â€ï¼Œä¸åŒ…å«ç®—æ³•ç»†èŠ‚ï¼Œä¹Ÿä¸åŒ…å«ä»»åŠ¡ç»†èŠ‚ã€‚

------

#### 2) `legged_lab/utils/task_registry.py`

**ï¼šä»»åŠ¡è·¯ç”±è¡¨ï¼ˆå­—ç¬¦ä¸² â†’ ç±»ä¸é…ç½®ï¼‰**

è¿™æ˜¯ä¸€ä¸ªå…¨å±€å­—å…¸ï¼š

- `"walk"` â†’ `env_class`ï¼ˆå“ªä¸ªç¯å¢ƒç±»ï¼‰
- `"walk"` â†’ `env_cfg`ï¼ˆç¯å¢ƒé…ç½®å¯¹è±¡ï¼‰
- `"walk"` â†’ `agent_cfg`ï¼ˆè®­ç»ƒé…ç½®å¯¹è±¡ï¼‰

å®ƒçš„ä»·å€¼æ˜¯è§£è€¦ï¼štrain.py æ°¸è¿œä¸éœ€è¦å†™æ­» `TienKungWalkFlatEnvCfg` è¿™ç§ç±»åï¼Œåªè¦ä¼ ä¸€ä¸ªå­—ç¬¦ä¸²å°±è¡Œã€‚

> æ³¨å†Œé€šå¸¸å‘ç”Ÿåœ¨ `legged_lab/envs/__init__.py` æˆ–å­æ¨¡å— import çš„ side-effect ä¸­ã€‚

------

#### 3) `legged_lab/envs/<tienkung>/...`

**ï¼šçœŸæ­£çš„ç¯å¢ƒå®ç°ï¼ˆMDP çš„â€œè¿è¡Œæ—¶å¼•æ“â€ï¼‰**

è¿™æ˜¯é¡¹ç›®â€œä»»åŠ¡å±‚â€çš„æ ¸å¿ƒï¼šæŠŠ cfg å˜æˆä¸€ä¸ªå¯äº¤äº’çš„ `VecEnv`ã€‚

å®è§‚ä¸Šï¼Œè¿™ä¸ªç¯å¢ƒç±»ä¼šå®ç°ï¼ˆæˆ–ç»„åˆå®ç°ï¼‰ï¼š

- **åœºæ™¯æ„å»º**ï¼šåŠ è½½æœºå™¨äºº articulationã€åœ°å½¢ã€ä¼ æ„Ÿå™¨ï¼ˆcontactã€height scanner ç­‰ï¼‰
- **çŠ¶æ€ä¸ç¼“å­˜**ï¼šobs historyã€action bufferã€episode buffersï¼ˆreset_buf/time_out_bufï¼‰
- **å‘½ä»¤ç”Ÿæˆ**ï¼šcommand_generator é‡‡æ ·é€Ÿåº¦/èˆªå‘æŒ‡ä»¤ï¼ˆå«ç«™ç«‹æ¯”ä¾‹ï¼‰
- **å¥–åŠ±ç®¡ç†**ï¼šæŠŠ `LiteRewardCfg` é‡Œæ¯ä¸ª term ç¼–è¯‘æˆå¯è®¡ç®—é¡¹ï¼ˆè°ƒç”¨ mdp.reward å‡½æ•°ï¼‰
- **ç»ˆæ­¢ä¸é‡ç½®**ï¼šæ¥è§¦ç»ˆæ­¢ã€è¶…æ—¶ç»ˆæ­¢ï¼›reset æ—¶æ‰§è¡Œ domain rand events
- **step æµç¨‹**ï¼šaction â†’ ç‰©ç†æ¨è¿›ï¼ˆdecimationï¼‰â†’ æ›´æ–°ä¼ æ„Ÿå™¨/çŠ¶æ€ â†’ obs/reward/done

ä½ å¯ä»¥æŠŠ env ç±»çœ‹æˆï¼š**MDP å®šä¹‰ + Isaac Sim æ‰§è¡Œ** çš„æ¡¥æ¢ã€‚

------

#### 4) legged_lab/mdp/ reward

 **æ–‡ä»¶ï¼šMDP ä¸­çš„â€œæ•°å­¦éƒ¨åˆ†â€**

è¿™ä¸€å±‚é€šå¸¸æ˜¯â€œçº¯å‡½æ•°å¼â€çš„ï¼šç»™å®š env çš„çŠ¶æ€å¼ é‡ï¼Œè®¡ç®—æŸä¸ª reward/penaltyã€‚

ä¾‹å¦‚ï¼š

- è·Ÿè¸ªé€Ÿåº¦ï¼š`exp(-||v_cmd - v||^2 / std^2)`
- å§¿æ€å¹³è¡¡ï¼šroll/pitch ç›¸å…³æƒ©ç½š
- æ¥è§¦çº¦æŸï¼šundesired contactsã€feet slideã€feet stumble
- åŠ¨ä½œå¹³æ»‘ï¼šaction_rateã€joint_acc
- ç«™ç«‹/è¡Œèµ°é—¨æ§ï¼š`zero_flag = ||cmd|| < eps`ï¼ˆå®ç°è¡Œä¸ºæ¨¡å¼åˆ‡æ¢ï¼‰
- gait clockï¼šç›¸ä½ maskï¼ˆswing/stanceï¼‰å¡‘å½¢æ­¥æ€

å®è§‚é‡è¦ç‚¹ï¼š
 **reward æ–‡ä»¶ä¸æ˜¯â€œç¯å¢ƒæ‰§è¡Œâ€ï¼Œè€Œæ˜¯â€œç›®æ ‡å‡½æ•°å®šä¹‰â€**ã€‚å®ƒå†³å®šç­–ç•¥æœ€ç»ˆå­¦æˆä»€ä¹ˆæ ·ã€‚

------

#### 5) `rsl_rl/runners/amp_on_policy_runner.py`

**è®­ç»ƒå¾ªç¯çš„å‘åŠ¨æœºï¼ˆé‡‡æ · + æ›´æ–° + æ—¥å¿—ï¼‰**

Runner è´Ÿè´£æŠŠ env å’Œç®—æ³•è¿èµ·æ¥ï¼Œæ ¸å¿ƒèŒè´£æ˜¯â€œæŠŠé—­ç¯è·‘èµ·æ¥â€ï¼š

- **Rollout**ï¼šå¾ªç¯ num_steps_per_env æ¬¡
  - è°ƒ actor å¾— action
  - env.step å¾— obs/reward/done
  - å­˜åˆ° rollout bufferï¼šobs, actions, rewards, dones, values, log_probs
- **Compute returns/advantages**ï¼šGAEï¼ˆgamma, lamï¼‰
- **ä¼˜åŒ–**ï¼šè°ƒç”¨ algorithm çš„ updateï¼ˆPPO/AMPPPOï¼‰
- **AMP æ‰©å±•ï¼ˆAmpOnPolicyRunner ç‰¹æœ‰ï¼‰**
  - åŒæ—¶ç»´æŠ¤ expert buffer / policy buffer
  - è®­ç»ƒ discriminator
  - è®¡ç®— AMP reward å¹¶æ··åˆåˆ° task rewardï¼ˆå— `amp_task_reward_lerp`ã€`amp_reward_coef` ç­‰æ§åˆ¶ï¼‰
- **æ—¥å¿—/ä¿å­˜**ï¼štensorboardã€checkpoint

ä½ å¯ä»¥æŠŠ runner ç†è§£ä¸ºï¼š**è®­ç»ƒç®¡çº¿çš„æµæ°´çº¿æ§åˆ¶å™¨**ã€‚

------

#### 6) `rsl_rl/algorithms/amp_ppo.py`

**AMPPPO çš„â€œæ•°å­¦æ›´æ–°è§„åˆ™â€**

è¿™æ˜¯æœ€â€œç®—æ³•å±‚â€çš„ä¸œè¥¿ï¼šå®šä¹‰ lossã€åå‘ä¼ æ’­ã€æ¢¯åº¦æ›´æ–°ã€‚

å®è§‚ç»„æˆé€šå¸¸æ˜¯ä¸‰å—ï¼ˆæ¯æ¬¡ updateï¼‰ï¼š

**6.1 PPO actor lossï¼ˆclipï¼‰**

- ratio = exp(new_logp - old_logp)
- clip ratio åˆ° [1-Îµ, 1+Îµ]
- å– min/æˆ– max å½¢æˆ clipped objective
- ç”¨ advantage åšæƒé‡

**6.2 PPO critic lossï¼ˆvalueï¼‰**

- MSE(return - value)
- å¸¸è§è¿˜æœ‰ clipped value lossï¼ˆä½  cfg é‡Œæ‰“å¼€äº†ï¼‰

**6.3 AMP discriminator loss + AMP reward**

- discriminator å­¦ä¼šåŒºåˆ† expert vs policyï¼ˆç±»ä¼¼ GAN çš„åˆ¤åˆ«å™¨ï¼‰
- ç”¨åˆ¤åˆ«å™¨è¾“å‡ºæ„é€ ä¸€ä¸ªâ€œåƒä¸“å®¶å°±é«˜åˆ†â€çš„ reward
- æœ€ç»ˆæ€» reward è¿›å…¥ advantage/returnï¼Œä»è€Œå½±å“ actor/critic æ›´æ–°

> è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ AMP èƒ½è®©åŠ¨ä½œâ€œæ›´åƒäººèµ°è·¯â€ï¼šå®ƒæŠŠâ€œåƒä¸“å®¶â€å˜æˆä¼˜åŒ–ç›®æ ‡çš„ä¸€éƒ¨åˆ†ã€‚

------

**æŠŠ 6 ä¸ªæ¨¡å—è¿æˆä¸€æ¡â€œæ¸…æ™°çš„å› æœé“¾â€**

ä½ å¯ä»¥è®°ä½è¿™æ¡é“¾ï¼š

**train.py**ï¼ˆå¯åŠ¨/æ‹¼è£…ï¼‰
 â†’ **task_registry**ï¼ˆæŠŠå­—ç¬¦ä¸²æ˜ å°„åˆ° env_class + cfgï¼‰
 â†’ **env_class**ï¼ˆæŠŠ cfg å˜æˆå¯ step çš„å¹¶è¡Œä»¿çœŸç¯å¢ƒï¼‰
 â†’ **mdp.reward**ï¼ˆenv åœ¨æ¯æ­¥è®¡ç®— reward term çš„æ•°å­¦å®šä¹‰ï¼‰
 â†’ **AmpOnPolicyRunner**ï¼ˆrollout + GAE + è°ƒç”¨ç®—æ³•æ›´æ–° + è®°å½•æ—¥å¿—ï¼‰
 â†’ **AMPPPO**ï¼ˆPPO loss + AMP åˆ¤åˆ«å™¨ lossï¼Œåšæ¢¯åº¦ä¸‹é™ï¼‰
 â†’ æ›´æ–°åçš„ policy å›åˆ° env å†é‡‡æ ·ï¼ˆå¾ªç¯ï¼‰

------

**ä½ ç°åœ¨æœ€è¯¥å…ˆæŠ“ä½çš„ 3 ä¸ªâ€œå®è§‚æŠ“æ‰‹â€**

1. **æ•°æ®ç»“æ„æŠ“æ‰‹**ï¼šrunner ä¸ env äº¤äº’çš„æœ€å…³é”®å››å…ƒç»„
   - obsï¼ˆpolicy è¾“å…¥ï¼‰
   - actionï¼ˆpolicy è¾“å‡ºï¼‰
   - rewardï¼ˆä¼˜åŒ–ç›®æ ‡ï¼‰
   - done/resetï¼ˆæ•°æ®åˆ†æ®µä¸ç»ˆæ­¢ï¼‰
2. **ä¸¤ç±»å¥–åŠ±æŠ“æ‰‹**ï¼štask reward vs AMP reward
   - task rewardï¼šè·Ÿè¸ªé€Ÿåº¦ã€ç¨³å®šã€ä¸æ‘”
   - AMP rewardï¼šåƒä¸“å®¶èµ°è·¯ï¼ˆé£æ ¼/è‡ªç„¶æ€§ï¼‰
3. **é—¨æ§æŠ“æ‰‹**ï¼šcommand å†³å®šâ€œç«™ç«‹ vs è¡Œèµ°â€çš„æ¨¡å¼åˆ‡æ¢
   - å¾ˆå¤š reward term ä¼šç”¨ `||cmd|| < eps` ä½œä¸ºå¼€å…³
   - æ‰€ä»¥ä¿®æ”¹ reward å°±èƒ½æ”¹å˜åˆ‡æ¢è¡Œä¸º





#### å¤©å·¥TensorBoard  Rewardå‚æ•°

##### **1ã€Action_rate_l2**

è¡¡é‡ **ç›¸é‚»ä¸¤ä¸ª timestep ä¹‹é—´ action å˜åŒ–çš„å¹³æ–¹å’Œ**ï¼š

![image-20251231110032553](/home/mig/Desktop/typora_images/Miniconda3-latest-Linux-x86_64.sh)

æƒ©ç½š **é«˜é¢‘æŠ–åŠ¨**

é¼“åŠ±åŠ¨ä½œå¹³æ»‘

é˜²æ­¢æ§åˆ¶ä¿¡å·â€œéœ‡è¡â€

##### **2ã€Ang_vel_xy_l2**

æ§åˆ¶èº«ä½“è§’é€Ÿåº¦ï¼ˆpitch/rollï¼‰æ¥è¿‘ç›®æ ‡ï¼ˆé€šå¸¸æ˜¯ 0ï¼‰ï¼š

![image-20251231110317710](/home/mig/snap/typora/110/.config/Typora/typora-user-images/image-20251231110317710.png)

ä¿è¯èº«ä½“ä¸æ‘‡ã€ä¸ç¿»

æ˜¯â€œå§¿æ€ç¨³å®šæ€§â€çš„æ ¸å¿ƒæŒ‡æ ‡

##### **3ã€Ankle_action & Ankle_torque**

**ï¼ˆè¸å…³èŠ‚åŠ¨ä½œä¸åŠ›çŸ©æƒ©ç½šï¼‰**

`ankle_action`ï¼šè¸å…³èŠ‚æ§åˆ¶è¾“å…¥å¹…åº¦

`ankle_torque`ï¼šå®é™…è¾“å‡ºåŠ›çŸ©

é˜²æ­¢è¸å…³èŠ‚å‰§çƒˆæ‘†åŠ¨

é¿å… unrealistic torque

##### 4ã€Body_orientation_l2

ï¼ˆèº«ä½“å§¿æ€åå·®ï¼‰

èº¯å¹²å§¿æ€ç›¸å¯¹ upright çš„åå·®å¹³æ–¹

é˜²æ­¢å‰å€¾/åä»°/ä¾§ç¿»

##### 5ã€Dof_acc_l2 & Dof_pos_limits

ï¼ˆå…³èŠ‚åŠ é€Ÿåº¦ & å…³èŠ‚æé™ï¼‰

`dof_acc_l2`ï¼šå…³èŠ‚åŠ é€Ÿåº¦å¹³æ»‘æ€§

`dof_pos_limits`ï¼šæ˜¯å¦æ¥è¿‘æœºæ¢°æé™



##### 6ã€Energy

åŠŸç‡æˆ–åŠ›çŸ©èƒ½è€—æƒ©ç½š

é¼“åŠ±èŠ‚èƒ½æ­¥æ€

##### 7ã€Feet_too_near

åŒè„šé—´è·è¿‡è¿‘çš„æƒ©ç½š

é˜²æ­¢ç»Šè„šã€äº¤å‰

##### 8ã€Feet_slide

è¶³åº•æ³•å‘åŠ›

æ°´å¹³æ»‘ç§»

##### 9ã€ Feet_stumble

è„šåœ¨éé¢„æœŸçŠ¶æ€ä¸‹ç¢°æ’æˆ–ç»Šå€’



#### å¤©å·¥TensorBoard  Losså‚æ•°

#####  Loss / amp

AMPï¼ˆAdversarial Motion Priorï¼‰é€šå¸¸æŠŠâ€œè®©ç­–ç•¥åŠ¨ä½œåƒ expertâ€çš„ç›®æ ‡å†™æˆä¸€ä¸ªå¯¹æŠ—å­¦ä¹ é—®é¢˜ï¼š

- ä¸€ä¸ª **åˆ¤åˆ«å™¨ D(s)** åŒºåˆ†â€œexpert è½¨è¿¹â€ä¸â€œpolicy è½¨è¿¹â€
- policy é€šè¿‡ AMP rewardï¼ˆæ¥è‡ª Dï¼‰è¢«é©±åŠ¨å»â€œéª—è¿‡åˆ¤åˆ«å™¨â€

`Loss/amp` ä¸€èˆ¬å°±æ˜¯ AMP åˆ¤åˆ«å™¨è®­ç»ƒçš„æ€» lossï¼ˆæˆ–å…¶ç›‘æ§æŒ‡æ ‡ï¼‰ï¼Œå¸¸ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š

![image-20251231112338575](/home/mig/snap/typora/110/.config/Typora/typora-user-images/image-20251231112338575.png)

å®ƒåæ˜  **åˆ¤åˆ«å™¨æ˜¯å¦åœ¨å­¦ä¹ **ã€æ˜¯å¦å·²ç»é¥±å’Œã€æ˜¯å¦å‡ºç°å´©åï¼ˆè¿‡å¼º/è¿‡å¼±ï¼‰ã€‚

**å…¸å‹å½¢æ€æ€ä¹ˆè¯»**

- **ä¸‹é™å¹¶è¶‹ç¨³**ï¼šåˆ¤åˆ«å™¨å­¦åˆ°äº†è¾ƒç¨³å®šçš„åŒºåˆ†è¾¹ç•Œï¼ˆæ­£å¸¸ï¼‰
- **é•¿æœŸæŒ¯è¡ä½†ä¸å‘æ•£**ï¼šå¯¹æŠ—è¾¾åˆ°åŠ¨æ€å¹³è¡¡ï¼ˆæ­£å¸¸/å¯æ¥å—ï¼‰
- **çªç„¶é£™å‡æˆ–å½’é›¶**ï¼šåˆ¤åˆ«å™¨è®­ç»ƒå‡ºé—®é¢˜ï¼ˆæ•°æ®ã€å­¦ä¹ ç‡ã€æ¢¯åº¦æƒ©ç½šã€å½’ä¸€åŒ–ç­‰ï¼‰

ä½ å½“å‰å›¾é‡Œ `amp` å¤§è‡´ç¨³å®šåœ¨ä¸€ä¸ªèŒƒå›´å†…å¹¶æŒ¯è¡ï¼Œé€šå¸¸æ„å‘³ç€ï¼š

> åˆ¤åˆ«å™¨è®­ç»ƒç¨³å®šï¼Œä½†å¯¹ policy æä¾›çš„â€œå¯ç”¨å­¦ä¹ ä¿¡å·â€å¯èƒ½å·²ç»å˜å¼±ï¼ˆé¥±å’Œï¼‰ã€‚





##### Loss / amp_expert_pred

è¿™æ˜¯â€œåˆ¤åˆ«å™¨å¯¹ expert æ ·æœ¬çš„è¾“å‡ºï¼ˆæˆ–å¯¹åº”æŸå¤±ï¼‰â€çš„ç›‘æ§ã€‚
 ä¸åŒå®ç°å¯èƒ½è®°å½•ï¼š

- **D(expert) çš„å¹³å‡è¾“å‡º**ï¼ˆè¶Šæ¥è¿‘â€œexpertâ€è¶Šå¥½ï¼‰
- æˆ– **expert åˆ†ç±»æŸå¤±**ï¼ˆè¶Šä½è¶Šå¥½ï¼‰

ä½ å†™çš„â€œExpert æ•°æ®è¢«åˆ¤åˆ«ä¸º expert çš„ç½®ä¿¡åº¦â€å±äºç¬¬ä¸€ç§è§£é‡Šã€‚

å®ƒå‘Šè¯‰ä½ ï¼š

- åˆ¤åˆ«å™¨æ˜¯å¦èƒ½æŠŠ expert çœ‹æˆ expertï¼ˆå¦åˆ™åˆ¤åˆ«å™¨åäº†ï¼‰
- expert æ•°æ®é¢„å¤„ç†/å½’ä¸€åŒ–æ˜¯å¦å¯¹é½ policy æ•°æ®ï¼ˆå¦åˆ™åˆ†å¸ƒæ¼‚ç§»ï¼‰

- **D(expert) å¾ˆé«˜ä¸”ç¨³å®š**ï¼šåˆ¤åˆ«å™¨è‡³å°‘èƒ½è¯†åˆ« expertï¼ˆæ­£å¸¸ï¼‰
- **D(expert) é€æ¸å˜å·®**ï¼šåˆ¤åˆ«å™¨è®­ç»ƒä¸ç¨³æˆ– expert æ•°æ®è¾“å…¥æ ¼å¼æœ‰é—®é¢˜
- **D(expert) è¿‡é¥±å’Œï¼ˆé•¿æœŸæç«¯å€¼ï¼‰**ï¼šåˆ¤åˆ«å™¨å¯èƒ½è¿‡å¼ºï¼Œå¯¼è‡´ policy æ¢¯åº¦â€œæ— ä¿¡æ¯â€



#####  Amp_grad_pen

WGAN-GP é£æ ¼çš„æ¢¯åº¦æƒ©ç½šé¡¹ï¼Œé˜²æ­¢åˆ¤åˆ«å™¨å˜å¾—è¿‡å°–é”å¯¼è‡´ä¸ç¨³å®šï¼š

å®ƒæ˜¯ AMP ç¨³å®šæ€§çš„â€œä¿é™©ä¸â€ã€‚

- å¤ªå°ï¼šåˆ¤åˆ«å™¨å¯èƒ½è¿‡åº¦å°–é”ï¼Œè®­ç»ƒä¼šæŠ–/å´©
- å¤ªå¤§ï¼šåˆ¤åˆ«å™¨è¢«å¼ºè¡Œå‹å¹³ï¼Œå­¦ä¹ èƒ½åŠ›ä¸è¶³ï¼Œpolicy å¾—ä¸åˆ°æœ‰æ•ˆä¿¡å·

æ€ä¹ˆè¯»

- ç¨³å®šåœ¨ä¸­ç­‰æ°´å¹³ï¼šé€šå¸¸æ­£å¸¸
- æŒç»­å¢å¤§ï¼šåˆ¤åˆ«å™¨æ¢¯åº¦åœ¨å˜â€œé‡â€ï¼Œå¯èƒ½å­¦ä¹ ç‡åå¤§ã€è¾“å…¥æœªå½’ä¸€åŒ–æˆ– reward/obs åˆ†å¸ƒæ¼‚ç§»
- æ¥è¿‘ 0 ä¸” amp_expert_pred/amp_policy_pred ä¹Ÿé¥±å’Œï¼šåˆ¤åˆ«å™¨å¯èƒ½è¢«â€œå‹å¹³ + é¥±å’Œâ€ï¼Œç»™ä¸å‡ºä¿¡æ¯





##### **Amp_policy_pred**

ç­–ç•¥æ ·æœ¬è¢«åˆ¤åˆ«æˆ expert çš„æ¦‚ç‡ï¼ˆè¶Šæ¥è¿‘ expert è¶Šå¥½ï¼‰

åˆ¤åˆ«å™¨å¯¹ policy æ ·æœ¬çš„è¾“å‡ºï¼ˆæˆ–å…¶æŸå¤±ï¼‰ã€‚

- **D(policy) è¶Šé«˜** â‡’ policy è¶Šåƒ expert

è¿™æ˜¯ AMP å¯¹ policy çš„æ ¸å¿ƒåé¦ˆï¼š

- å¦‚æœ policy è¶Šæ¥è¶Šåƒ expertï¼Œè¿™ä¸ªæŒ‡æ ‡åº”é€æ­¥â€œå‘ expert æ–¹å‘ç§»åŠ¨â€
- å¦‚æœé•¿æœŸä¸åŠ¨ï¼Œè¯´æ˜ AMP reward å¯¹ policy å·²ç»æ²¡æœ‰é©±åŠ¨åŠ›ï¼ˆæˆ–è¢«å…¶ä»–æƒ©ç½šé¡¹å‹ä½ï¼‰

- **æŒç»­æ”¹å–„å¹¶è¶‹ç¨³**ï¼šAMP åœ¨å‘æŒ¥ä½œç”¨

- **æ—©æœŸæ”¹å–„ï¼ŒåæœŸå¹³å°**ï¼šAMP ä¿¡å·é¥±å’Œ/è¢« PPO ç¨³å®šé¡¹é”æ­»

- **åæœŸåå‘æ¶åŒ–**ï¼špolicy è¢« task reward/penalty æ‹‰åï¼ŒAMP æ‹‰ä¸å›æ¥ï¼ˆå¸¸è§äºä½ è¿™ç§ reward æƒ©ç½šè¿‡å¼ºï¼‰

  

**Entropy** 

ç­–ç•¥åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§ã€‚å¯¹é«˜æ–¯ç­–ç•¥ï¼ˆè¿ç»­åŠ¨ä½œï¼‰ï¼š

![image-20251231113531254](/home/mig/snap/typora/110/.config/Typora/typora-user-images/image-20251231113531254.png)

å¾ˆå¤š logger è®°å½•çš„æ˜¯ entropy æœ¬èº«ï¼Œä¹Ÿå¯èƒ½è®°å½• `-entropy`ï¼ˆçœ‹å®ç°ï¼‰ã€‚
 ä½ å›¾é‡Œä»é«˜åˆ°ä½å¿«é€Ÿä¸‹é™ï¼Œå¹¶åœ¨è¾ƒä½å€¼å¹³å°ï¼Œç¬¦åˆâ€œæ¢ç´¢å¿«é€Ÿæ¶ˆå¤±â€ã€‚

entropy æ˜¯ PPO çš„â€œæ¢ç´¢æ¸©åº¦è®¡â€ï¼š

- **å¤ªé«˜**ï¼šç­–ç•¥å¤ªéšæœºï¼Œå­¦ä¸ç¨³

- **å¤ªä½**ï¼šç­–ç•¥è¿‡æ—©ç¡®å®šï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜å¹¶åœæ­¢æ”¹è¿›ï¼ˆä½ ç°åœ¨çš„å…¸å‹çŠ¶æ€ï¼‰

  

- åˆæœŸä¸‹é™ï¼šæ­£å¸¸ï¼ˆä»éšæœºæ¢ç´¢èµ°å‘æ”¶æ•›ï¼‰
- ä¸‹é™åä»ç¼“æ…¢å˜åŒ–ï¼šå¥åº·ï¼ˆè¿˜èƒ½ç»§ç»­å¾®è°ƒï¼‰
- **è¿‡æ—©é™åˆ°å¾ˆä½ä¸”é•¿æœŸå¹³**ï¼šç­–ç•¥å†»ç»“ï¼ˆPPO æ›´æ–°å¹…åº¦å°ã€KL å°ã€ä¼˜åŠ¿å°ã€lr å°ï¼‰



#####  Learning_rate

ä¼˜åŒ–å™¨å½“å‰ LRï¼ˆå¯èƒ½æ¥è‡ª scheduleï¼‰ã€‚

LR å†³å®š PPO æ›´æ–°â€œèƒ½ä¸èƒ½åŠ¨èµ·æ¥â€ã€‚å¦‚æœä½ è¿™é‡Œå‡ ä¹æ‰åˆ° 0ï¼š

> é‚£ä¹ˆåé¢å³ä½¿ reward æœ‰ä¿¡å·ï¼Œç½‘ç»œå‚æ•°ä¹ŸåŸºæœ¬ä¸ä¼šæ›´æ–°ã€‚

- `constant` æˆ–ç¼“æ…¢ decayï¼šå¸¸è§
- **å¾ˆå¿«è¡°å‡åˆ°æ¥è¿‘ 0**ï¼šå¸¸è§è¯¯é…ç½®ï¼ˆscheduler å¤ªæ¿€è¿›ã€æ€»æ­¥æ•°è®¾é”™ã€warmup/anneal é€»è¾‘æœ‰ bugï¼‰

å·¥ç¨‹ä¸Šï¼Œå¦‚æœä½ æƒ³ç»§ç»­å­¦ä¹ ï¼Œé€šå¸¸åº”è¯¥è®¾ç½®ï¼š

- LR ä¸‹é™ï¼ˆfloorï¼‰ï¼Œä¾‹å¦‚ `min_lr=3e-5`

- æˆ–ç›´æ¥ constant LRï¼ˆè‡³å°‘åœ¨æ—©æœŸï¼‰

  

##### Surrogate

PPO çš„æ ¸å¿ƒç›®æ ‡ï¼ˆclipped surrogate objectiveï¼‰ï¼Œä¸€èˆ¬ç›‘æ§çš„æ˜¯ **è´Ÿçš„ç›®æ ‡å€¼** æˆ–å…¶å‡å€¼ï¼š

logger è‹¥è®°å½•ä¸º `loss/surrogate`ï¼Œå¸¸è§æ˜¯æŠŠè¦æœ€å¤§åŒ–çš„ç›®æ ‡å–è´Ÿå·åå˜æˆâ€œlossâ€ã€‚

**ä¸ºä»€ä¹ˆé‡è¦**

å®ƒç›´æ¥åæ˜ ï¼š

- ä¼˜åŠ¿ $A_t$ æ˜¯å¦æœ‰ä¿¡å·
- policy æ›´æ–°æ˜¯å¦åœ¨å‘ç”Ÿ
- clip æ˜¯å¦é¢‘ç¹è§¦å‘ï¼ˆæ›´æ–°å¤ªå¤§/å¤ªå°ï¼‰

**æ€ä¹ˆè¯»**

- æœ‰æ³¢åŠ¨ä¸”æœ‰è¶‹åŠ¿ï¼šåœ¨å­¦ä¹ 
- **é•¿æœŸç¨³å®šåœ¨ä¸€ä¸ªå°å¹…åŒºé—´**ï¼špolicy æ›´æ–°å¾ˆå°ã€ä¼˜åŠ¿å¾ˆå°ã€æˆ– LR å¤ªå°/entropy å¤ªä½å¯¼è‡´å†»ç»“
- å¦‚æœä½ åŒæ—¶çœ‹åˆ° entropy ä½ã€lr ä½ï¼Œé‚£ surrogate ç¨³å®šåŸºæœ¬å°±æ˜¯â€œæ²¡åœ¨å­¦â€





**Value_function**



ä»·å€¼å‡½æ•°æ‹Ÿåˆè¯¯å·®ï¼ˆMSE/Huberï¼‰ï¼š
$$
L_V = \mathbb{E}\big[(V_\phi(s_t) - \hat{R}_t)^2\big]
$$


**ä¸ºä»€ä¹ˆé‡è¦**

critic è´¨é‡å†³å®š advantage è´¨é‡ï¼Œè¿›è€Œå†³å®š policy æ›´æ–°æ–¹å‘æ˜¯å¦æ­£ç¡®ã€‚

**æ€ä¹ˆè¯»**

- åˆæœŸè¾ƒé«˜ï¼Œéšåä¸‹é™æˆ–è¶‹ç¨³ï¼šæ­£å¸¸
- **æŒç»­ä¸Šå‡**ï¼šå¸¸è§åŸå› ï¼š
  1. å›æŠ¥åˆ†å¸ƒåœ¨å˜ï¼ˆreward shapingã€terminationã€AMP reward scale å˜åŒ–ï¼‰
  2. critic å­¦ä¹ ç‡/å®¹é‡ä¸å¤Ÿ
  3. policy å†»ç»“å¯¼è‡´é‡‡æ ·åˆ†å¸ƒå•ä¸€ï¼Œä½† returns å™ªå£°ä»åœ¨ï¼ˆcritic ä¼šâ€œå­¦ä¸å‡†â€ï¼‰
  4. å½’ä¸€åŒ–ï¼ˆobs/reward/advantageï¼‰ä¸ä¸€è‡´

ä½ ä¹‹å‰çš„å›¾é‡Œ value_function æœ‰ä¸Šå‡è¶‹åŠ¿ï¼Œè¿™é€šå¸¸æç¤ºï¼š

> ä¸æ˜¯æ•°å€¼çˆ†ç‚¸ï¼Œè€Œæ˜¯â€œcritic æ­£åœ¨è·Ÿä¸ä¸Š returns çš„ç»“æ„â€ï¼Œå¸¸ç”± reward å¤±è¡¡ + policy å†»ç»“å¼•å‘ã€‚







#### å¤©å·¥TensorBoard  Perfå‚æ•°

##### **Perf / collection_time**

**é‡‡æ ·æ—¶é—´ï¼ˆç¯å¢ƒäº¤äº’è€—æ—¶ï¼‰**

ğŸ“ˆ ä½ å›¾ä¸­è¡¨ç°ï¼š

- ç¨³å®šåœ¨ **â‰ˆ 0.41 s**
- æ³¢åŠ¨å¾ˆå°ï¼Œæ²¡æœ‰è¶‹åŠ¿æ€§å¢é•¿æˆ–ä¸‹é™



##### Perf / **learning_time**

**å•æ¬¡æ›´æ–°ï¼ˆåå‘ä¼ æ’­ï¼‰è€—æ—¶**

ğŸ“ˆ ä½ å›¾ä¸­ï¼š

- ç¨³å®šåœ¨ ~0.061s
- æ³¢åŠ¨æå°

è§£é‡Šï¼š

- ç½‘ç»œè§„æ¨¡ã€batch sizeã€optimizer éƒ½æ˜¯ç¨³å®šçš„
- GPU æ²¡æœ‰è¿‡è½½
- æ²¡æœ‰å‡ºç°æ¢¯åº¦çˆ†ç‚¸ / NaN

#####  **total_fps**

**æ•´ä½“è®­ç»ƒé€Ÿåº¦ï¼ˆenv step / secondï¼‰**

ğŸ“ˆ ä½ å›¾ä¸­ï¼š

- å¤§çº¦åœ¨ 3200â€“3300 fps æ³¢åŠ¨
- ç¨³å®šï¼Œæ²¡æœ‰æ˜æ˜¾ä¸‹é™

è§£é‡Šï¼š

- å¹¶è¡Œç¯å¢ƒæ•°åˆç†
- æ²¡æœ‰æ€§èƒ½ç“¶é¢ˆ



#### å¤©å·¥TensorBoard  Trainå‚æ•°

##### Train / mean_episode_length

ï¼ˆå¹³å‡ episode é•¿åº¦ï¼‰

ğŸ“ˆ èµ°åŠ¿ï¼š

- ä» ~65 æŒç»­ä¸Šå‡åˆ° ~81
- ç¨³å®šå¢é•¿ï¼Œæ²¡æœ‰å›è½

è§£è¯»ï¼š

- agent è¶Šæ¥è¶Šä¸å®¹æ˜“â€œå¤±è´¥â€
- episode æ›´é•¿

##### Train / mean_episode_length_time

è¿™æ˜¯ episode çš„**çœŸå®æ—¶é—´é•¿åº¦ï¼ˆç§’ï¼‰**ã€‚

ğŸ“ˆ ä» ~80s â†’ ~85s

è¯´æ˜ï¼š

- ä¸ step æ•°ä¸€è‡´å¢é•¿
- ä¸æ˜¯ simulation bug



##### Train / mean_reward

ğŸ“ˆ ä½ è¿™æ¡éå¸¸å…³é”®ï¼š

- ä»å¤§çº¦ **-1.2 â†’ -0.3**
- æŒç»­ä¸Šå‡ï¼Œä½†å§‹ç»ˆä¸ºè´Ÿ
- åæœŸä¸Šå‡æ˜æ˜¾å˜æ…¢

è§£é‡Šï¼š

- reward ç¡®å®åœ¨å˜å¥½ï¼ˆæ¨¡å‹åœ¨å­¦ï¼‰
- ä½†è¢« **è´Ÿé¡¹ï¼ˆæƒ©ç½šï¼‰ä¸»å¯¼**
- æ°¸è¿œâ€œèµšä¸åˆ°æ­£æ”¶ç›Šâ€

è¿™å’Œä½  reward é‡Œï¼š

- energy
- dof limits
- ankle torque
   ç­‰é¡¹è¿‡å¼ºå®Œå…¨ä¸€è‡´ã€‚



##### Train / mean_reward_time

è¿™æ¡æ˜¯ time-weighted rewardï¼ˆéšæ—¶é—´ç´¯è®¡ï¼‰

è¡¨ç°ï¼š

- è¶‹åŠ¿ç±»ä¼¼
- æŠ–åŠ¨æ›´æ˜æ˜¾ï¼ˆå› ä¸º reward æ³¢åŠ¨ï¼‰

è¯´æ˜ï¼š

- æ¨¡å‹åœ¨æŸäº›é˜¶æ®µå°è¯•â€œèµ°â€ï¼Œä½†å¾ˆå¿«è¢«æƒ©ç½šæ‹‰å›









# å¼ºåŒ–å­¦ä¹  

## PPOç®—æ³•

##### GAE

**GAEï¼ˆGeneralized Advantage Estimationï¼Œå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰**
 æ˜¯ä¸€ç§ **ç”¨ TD æ®‹å·®çš„æŒ‡æ•°åŠ æƒå’Œæ¥ä¼°è®¡ä¼˜åŠ¿å‡½æ•° A(s_t,a_t)** çš„æ–¹æ³•ï¼Œç”¨æ¥åœ¨**é«˜æ–¹å·®ï¼ˆMonte Carloï¼‰**å’Œ**é«˜åå·®ï¼ˆTDï¼‰**ä¹‹é—´åšè¿ç»­å¯è°ƒçš„æŠ˜ä¸­ã€‚

###  Vã€Qã€Aã€r çš„å«ä¹‰ä¸å…³ç³»

åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆå°¤å…¶æ˜¯ PPO / Actorâ€“Critic æ–¹æ³•ï¼‰ä¸­ï¼Œ**rã€Vã€Qã€A** æ˜¯æœ€æ ¸å¿ƒã€ä½†ä¹Ÿæœ€å®¹æ˜“æ··æ·†çš„å››ä¸ªæ¦‚å¿µã€‚ä¸‹é¢ä»**å®šä¹‰ â†’ æ•°å­¦å½¢å¼ â†’ ç›´è§‰ç†è§£ â†’ ä»£ç å¯¹åº”**å››ä¸ªå±‚é¢ç»Ÿä¸€è¯´æ˜ã€‚

------

####  r 

**â€”â€” Rewardï¼ˆå³æ—¶å¥–åŠ±ï¼‰**

**æ•°å­¦å®šä¹‰**
$$
r_t = r(s_t, a_t, s_{t+1})
$$
**å«ä¹‰**

> **ç¯å¢ƒåœ¨å½“å‰æ—¶é—´æ­¥ç»™å‡ºçš„å³æ—¶åé¦ˆ**

- æ¥è‡ªç¯å¢ƒï¼Œè€Œéæ¨¡å‹å­¦ä¹ 
- åªåæ˜ â€œå½“å‰è¿™ä¸€æ­¥â€çš„å¥½å
- ä¸åŒ…å«å¯¹æœªæ¥çš„ç›´æ¥åˆ¤æ–­

**ç›´è§‰ç†è§£**

> â€œæˆ‘åˆšåˆšè¿™ä¸€æ­¥ï¼Œç¯å¢ƒç»™æˆ‘æ‰“äº†å¤šå°‘åˆ†ï¼Ÿâ€

CartPole ä¸­

- æ¯ä¸€æ­¥æœªå€’ï¼š`r = 1`
- æ†å€’æˆ–è¶Šç•Œï¼šepisode ç»“æŸ

PPO ä»£ç ä¸­çš„ä½ç½®

```
next_obs, reward, terminated, truncated, _ = env.step(action)
```

è¿™é‡Œçš„ `reward` å³ $r_t$ã€‚

------

#### V 

**â€”â€” State Value Functionï¼ˆçŠ¶æ€ä»·å€¼å‡½æ•°ï¼‰**

**æ•°å­¦å®šä¹‰**
$$
V^\pi(s) = \mathbb{E}_\pi\!\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k} \;\middle|\; s_t = s\right]
$$
**å«ä¹‰**

> **ç«™åœ¨çŠ¶æ€ $s$ ä¸Šï¼Œåœ¨å½“å‰ç­–ç•¥ $\pi$ ä¸‹ï¼Œæœªæ¥è¿˜èƒ½æœŸæœ›è·å¾—å¤šå°‘ç´¯è®¡å›æŠ¥**

- åªä¸çŠ¶æ€æœ‰å…³ï¼Œä¸æŒ‡å®šåŠ¨ä½œ
- æ˜¯ä¸€ä¸ªæœŸæœ›å€¼ï¼ˆå¹³å‡æ„ä¹‰ï¼‰
- ç”± Critic ç½‘ç»œè¿‘ä¼¼å­¦ä¹ 

**ç›´è§‰ç†è§£**

> â€œè¿™ä¸ªä½ç½®æ•´ä½“ä¸Šå€¼ä¸å€¼ï¼Ÿâ€

PPO **ä»£ç ä¸­çš„ä½ç½®**

```
logits, value = model(obs)
```

å…¶ä¸­ `value` å³ $V(s_t)$ã€‚

------

#### Q 

**â€”â€” Action-Value Functionï¼ˆåŠ¨ä½œä»·å€¼å‡½æ•°ï¼‰**

**æ•°å­¦å®šä¹‰**
$$
Q^\pi(s,a) = \mathbb{E}_\pi\!\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k} 
\;\middle|\; s_t = s,\; a_t = a\right]
$$
**å«ä¹‰**

> **åœ¨çŠ¶æ€ $s$ ä¸‹ï¼Œå¦‚æœå½“å‰è¿™ä¸€æ­¥é€‰æ‹©åŠ¨ä½œ $a$ï¼Œæœªæ¥æœŸæœ›èƒ½è·å¾—å¤šå°‘ç´¯è®¡å›æŠ¥**

- åŒæ—¶ä¾èµ–çŠ¶æ€å’ŒåŠ¨ä½œ
- ç²’åº¦æ¯” V æ›´ç»†
- åœ¨ DQN ç­‰æ–¹æ³•ä¸­ä¼šè¢«æ˜¾å¼å»ºæ¨¡

**ç›´è§‰ç†è§£**

> â€œåœ¨è¿™ä¸ªä½ç½®ï¼Œèµ°è¿™ä¸€æ­¥å€¼ä¸å€¼ï¼Ÿâ€

**åœ¨ PPO ä¸­å¦‚ä½•å¾—åˆ° Qï¼Ÿ**

PPO **ä¸æ˜¾å¼å­¦ä¹  Q ç½‘ç»œ**ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€æ­¥ bootstrap è¿‘ä¼¼ï¼š
$$
Q(s_t,a_t) \;\approx\; r_t + \gamma(1-d_t)V(s_{t+1})
$$




#### è¯æ˜

**æŠŠâ€œæœªæ¥å›æŠ¥â€æ‹†æˆâ€œç¬¬ä¸€æ­¥ + å‰©ä½™éƒ¨åˆ†â€**

å¯¹æ— ç©·å’Œåšä»£æ•°æ‹†åˆ†ï¼š
$$
\sum_{k=0}^{\infty}\gamma^k r_{t+k}
=
r_t
+
\gamma \sum_{k=0}^{\infty}\gamma^k r_{t+1+k}
$$
è¿™æ˜¯**çº¯ä»£æ•°æ’ç­‰å¼**ã€‚

ä»£å› Q çš„å®šä¹‰ï¼š
$$
Q^\pi(s_t,a_t)
=
\mathbb{E}_\pi
\!\left[
r_t
+
\gamma \sum_{k=0}^{\infty}\gamma^k r_{t+1+k}
\;\middle|\;
s_t,a_t
\right]
$$
ç°åœ¨å…³æ³¨ååŠé¡¹ï¼š

é€šè¿‡å…¬å¼å¯ä»¥è¯æ˜
$$
\mathbb{E}_\pi
\!\left[
\sum_{k=0}^{\infty}\gamma^k r_{t+1+k}
\;\middle|\;
s_{t+1}
\right]
=
V^\pi(s_{t+1})
$$


**ç†è®ºå±‚é¢ï¼ˆä¸¥æ ¼ï¼‰**
$$
Q^\pi(s,a)
=
\mathbb{E}\big[
r + \gamma V^\pi(s')
\big]
\qquad \text{ï¼ˆä¸¥æ ¼æˆç«‹ï¼‰}
$$

------

**ç®—æ³•å±‚é¢ï¼ˆPPO / Actorâ€“Criticï¼‰**

åœ¨å®é™…ç®—æ³•ä¸­ï¼š

- çœŸå®çš„ $V^\pi$ ä¸å¯å¾—
- åªèƒ½ä½¿ç”¨ä¸€ä¸ªå­¦ä¹ åˆ°çš„ Critic ç½‘ç»œï¼š

$$
V_\phi(s) \approx V^\pi(s)
$$

äºæ˜¯ï¼Œåœ¨**æ ·æœ¬å±‚é¢**ä½¿ç”¨ï¼š
$$
Q(s_t,a_t)
\;\approx\;
r_t + \gamma V_\phi(s_{t+1})
$$










------

#### A 

â€”â€” **Advantage** Functionï¼ˆä¼˜åŠ¿å‡½æ•°ï¼‰

**æ•°å­¦å®šä¹‰**
$$
A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
$$
**å«ä¹‰ï¼ˆPPO çš„æ ¸å¿ƒï¼‰**

> **åœ¨å½“å‰çŠ¶æ€ä¸‹ï¼Œé€‰è¿™ä¸ªåŠ¨ä½œæ¯”â€œå¹³å‡åŠ¨ä½œâ€å¥½å¤šå°‘**

- æ˜¯ä¸€ä¸ªç›¸å¯¹é‡
- $A > 0$ï¼šå¥½äºå¹³å‡ï¼Œåº”è¯¥æé«˜æ¦‚ç‡
- $A < 0$ï¼šå·®äºå¹³å‡ï¼Œåº”è¯¥é™ä½æ¦‚ç‡

**ä¸ºä»€ä¹ˆç­–ç•¥æ¢¯åº¦ç”¨** Aï¼Ÿ

- ç›´æ¥ç”¨ Q â†’ æ–¹å·®å¤§
- å‡å» V ä½œä¸º baseline â†’ **ä¸æ”¹å˜æœŸæœ›æ¢¯åº¦ï¼Œä½†æ˜¾è‘—é™ä½æ–¹å·®**

**PPO ä»£ç ä¸­çš„ä½ç½®**

ä¼˜åŠ¿ç”± TD æ®‹å·® + GAE è®¡ç®—ï¼š

```
buffer.compute_gae(...)
adv_b = buffer.advantages
```

------

#### å››è€…ä¹‹é—´çš„å› æœå…³ç³»

```
ç¯å¢ƒåé¦ˆï¼š        r_t
                   â†“
Critic ä¼°è®¡ï¼š   V(s_t), V(s_{t+1})
                   â†“
ä¸€æ­¥ Q è¿‘ä¼¼ï¼š   Q(s_t,a_t) â‰ˆ r_t + Î³ V(s_{t+1})
                   â†“
ä¼˜åŠ¿å®šä¹‰ï¼š     A(s_t,a_t) = Q - V(s_t)
```

------

#### åœ¨ PPO ä¸­å„è‡ªçš„è§’è‰²åˆ†å·¥

| é‡   | æ¥æº        | ä½œç”¨                 |
| ---- | ----------- | -------------------- |
| r    | ç¯å¢ƒ        | æä¾›çœŸå®å³æ—¶åé¦ˆ     |
| V    | Critic ç½‘ç»œ | baseline + bootstrap |
| Q    | éšå¼è¿‘ä¼¼    | æ„é€ ä¼˜åŠ¿             |
| A    | GAE è®¡ç®—    | Actor æ›´æ–°æ–¹å‘       |

------

#### ä¸€å¥è¯ç»ˆæè®°å¿†æ³•

> **r æ˜¯å½“ä¸‹å‘ç”Ÿçš„äº‹ï¼Œ
>  V æ˜¯ç«™åœ¨è¿™é‡Œçš„æ€»ä½“æœŸæœ›ï¼Œ
>  Q æ˜¯èµ°è¿™ä¸€æ­¥åçš„é•¿æœŸä»·å€¼ï¼Œ
>  A æ˜¯â€œè¿™ä¸€æ­¥æ¯”å¹³å‡å¥½å¤šå°‘â€ã€‚**















































