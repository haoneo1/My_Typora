

# ç°åœ¨éœ€è¦è§£å†³çš„é—®é¢˜

#### 2026/1/20

1ã€ææ¸…æ¥šrun_cfg æ–‡ä»¶é‡Œé¢çš„å†…å®¹

2ã€exp ä¸ºä»€ä¹ˆä¼šè¶‹è¿‘å¹³ç¼“ 

3ã€PDå¢ç›Šæ˜¯ä»€ä¹ˆ

4ã€ è‹±è¯­å­¦ä¹ 

5ã€æš´å¸ˆå…„ä»£ç 

6ã€å¿ƒè„ç›¸å…³è®ºæ–‡

6ã€proçš„èµ°ç«™ä¸€ä½“







sh buildwebots.sh   ä½¿ç”¨è¿™ä¸ªè°ƒç”¨è™šæ‹Ÿç¯å¢ƒ
sh buildrobot.sh    è°ƒç”¨ä¸Šæœºç¯å¢ƒ

# ä»£ç å­¦ä¹ 

### conda å‘½ä»¤ 

conda deactivate
conda env remove -n TianG

conda create -n TianG python=3.10 -y
conda activate TianG
pip install -U pip



g++ -std=c++17 -Wall -Wextra -Wpedantic -g src/hello.cpp -o build/hello ä¸¥æ ¼è°ƒè¯•C++


tree -L 3  æŸ¥çœ‹3çº§ç›®å½•



### ros2 å‘½ä»¤è¡Œ

åˆ‡æ¢ros

source /opt/ros/noetic/setup.bash 

source /opt/ros/humble/setup.bash


colcon build ä¼šå°†æ„å»ºç»“æœå®‰è£…åˆ°ä¸€ä¸ªç›®å½•ä¸­ï¼Œé»˜è®¤æ˜¯åœ¨ install/ ç›®å½•ä¸‹ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æºå®‰è£…è·¯å¾„æ¥ä½¿å¾—å·¥ä½œç©ºé—´ä¸­çš„æ‰€æœ‰å¯æ‰§è¡Œæ–‡ä»¶ã€åº“æ–‡ä»¶å’Œ Python åŒ…å¯ç”¨ï¼š 

source install/setup.bash

ros2 node list æŸ¥çœ‹å½“å‰çš„node
ros2 node info /turtlesim æŸ¥çœ‹è¿™ä¸ªnodeä¸‹çš„è®¢é˜… æœåŠ¡

ros2 topic list æŸ¥çœ‹å½“å‰çš„topicæœ‰å“ªäº›
ros2 topic echo /turtle/pose  ä¸ºä¾‹ æ‰“å°å‡ºå½“å‰çš„è¯é¢˜é‡Œé¢çš„å†…å®¹

ros2 bag record /turtle1/cmd_vel ï¼ˆè¯é¢˜ï¼‰ å¼€å§‹è®°å½•æŸä¸ªè¯é¢˜çš„æ•°æ®
ros2 bag play rosbag2_2025_12_30-15_36_29/(æ–‡ä»¶)  æ’­æ”¾è®°å½•çš„æ•°æ®

1d27ecf38583de0c0dc337bd0805f6ca

ros2 pkg create --build-type ament_cmake learning_package_c  åˆ›å»ºC++åŠŸèƒ½åŒ…
ros2 pkg create --build-type ament_python learning_package_py  åˆ›å»ºpythonåŠŸèƒ½åŒ…



### C++å‘½ä»¤



### ä¸Šæœºå‘½ä»¤

**transtojit.pyè½¬æ¢**







**webotsä»¿çœŸ**

```
 webots webots/worlds/tg2_evt_lite.wbt
```



### gitå‘½ä»¤

#### åˆ†æ”¯é”™è¯¯

**1. ç°è±¡**

- GitLab ç½‘é¡µå·¦ä¸Šè§’é€‰æ‹©äº†åˆ†æ”¯ `tg2.0_lite`ï¼Œç½‘é¡µé‡Œèƒ½çœ‹åˆ°å®Œæ•´æ–‡ä»¶ã€‚
- æœ¬åœ°æ‰§è¡Œï¼š
  ```bash
  git clone git@git.x-humanoid-cloud.com:motion-intelligence-group/mig-tg2/tk_lab.git

ä¸‹è½½ä¸‹æ¥åªæœ‰ `README`ï¼Œå†…å®¹ä¸ `main` åˆ†æ”¯ä¸€è‡´ã€‚

**2. åŸå› ï¼ˆå…³é”®ç‚¹ï¼‰**

- `git clone` **é»˜è®¤åªä¼šæ£€å‡ºè¿œç«¯çš„â€œé»˜è®¤åˆ†æ”¯â€**ï¼ˆGitLab é¡µé¢æ ‡æ³¨ä¸ºâ€œé»˜è®¤â€çš„åˆ†æ”¯ï¼Œä¸€èˆ¬æ˜¯ `main`ï¼‰ã€‚
- GitLab ç½‘é¡µåˆ‡æ¢åˆ†æ”¯ **åªå½±å“ç½‘é¡µæ˜¾ç¤º**ï¼Œä¸ä¼šå½±å“ä½ æœ¬åœ°ä»“åº“çŠ¶æ€ã€‚

------

**3. æ­£ç¡®åšæ³• Aï¼šç›´æ¥ clone æŒ‡å®šåˆ†æ”¯ï¼ˆæ¨èï¼‰**

```
git clone -b tg2.0_lite \
  git@git.x-humanoid-cloud.com:motion-intelligence-group/mig-tg2/tk_lab.git
```

------

**4. æ­£ç¡®åšæ³• Bï¼šå·² clone çš„æƒ…å†µä¸‹åˆ‡æ¢åˆ° `tg2.0_lite`**

è¿›å…¥ä»“åº“ç›®å½•ï¼š

```
cd tk_lab
```

æ‹‰å–è¿œç«¯åˆ†æ”¯ä¿¡æ¯ï¼š

```
git fetch origin
```

æŸ¥çœ‹è¿œç«¯åˆ†æ”¯ï¼š

```
git branch -a
```

åˆ‡æ¢åˆ†æ”¯ï¼ˆæ¨èç”¨ `switch`ï¼‰ï¼š

```
git switch tg2.0_lite
```

å¦‚æœæœ¬åœ°æ²¡æœ‰è¯¥åˆ†æ”¯ï¼Œä»è¿œç«¯åˆ›å»ºæœ¬åœ°åˆ†æ”¯ï¼š

```
git switch -c tg2.0_lite origin/tg2.0_lite
```

------

**5. éªŒè¯æ˜¯å¦æˆåŠŸï¼ˆå¿…é¡»ï¼‰**

```
git branch
```

åº”çœ‹åˆ°ï¼š

```
* tg2.0_lite
```

æ£€æŸ¥æœ€è¿‘æäº¤ï¼š

```
git log --oneline --max-count=3
```

æäº¤è®°å½•åº”ä¸ GitLab ç½‘é¡µ `tg2.0_lite` åˆ†æ”¯ä¸€è‡´ã€‚

------



#### Git æäº¤å¹¶åŒæ­¥åˆ°è¿œç«¯

**1) `git add -A`**

- ä½œç”¨ï¼šæŠŠå½“å‰ç›®å½•é‡Œ**æ‰€æœ‰æ”¹åŠ¨**åŠ å…¥â€œæš‚å­˜åŒºï¼ˆstaging areaï¼‰â€
- åŒ…æ‹¬ï¼š**æ–°å¢æ–‡ä»¶ã€ä¿®æ”¹æ–‡ä»¶ã€åˆ é™¤æ–‡ä»¶**
- å«ä¹‰ï¼šå‘Šè¯‰ Gitã€Œè¿™äº›å˜åŒ–æˆ‘å‡†å¤‡æäº¤ã€

**2) `git commit -m "update notes"`**

- ä½œç”¨ï¼šæŠŠæš‚å­˜åŒºé‡Œçš„å†…å®¹**æ‰“åŒ…æˆä¸€æ¬¡æäº¤ï¼ˆcommitï¼‰**ï¼Œç”Ÿæˆä¸€æ¡æäº¤è®°å½•
- `-m` åé¢æ˜¯æäº¤è¯´æ˜ï¼ˆå»ºè®®å†™æ¸…æ¥šæ”¹äº†ä»€ä¹ˆï¼‰
- å«ä¹‰ï¼šåœ¨æœ¬åœ°ä»“åº“é‡Œä¿å­˜ä¸€ä¸ªâ€œç‰ˆæœ¬å¿«ç…§â€

**3) `git push`**

- ä½œç”¨ï¼šæŠŠæœ¬åœ°çš„ commit **æ¨é€åˆ°è¿œç«¯ä»“åº“ï¼ˆGitHub/GitLabï¼‰**
- è¿œç«¯ç½‘é¡µåªæœ‰åœ¨ä½  push äº†æ–°çš„ commit åæ‰ä¼šæ›´æ–°
- å«ä¹‰ï¼šåŒæ­¥åˆ°æœåŠ¡å™¨ï¼Œè®©åˆ«äººä¹Ÿèƒ½çœ‹åˆ°ä½ çš„æ›´æ–°

æ€»ç»“ï¼š

```bash
git status        # çœ‹æ”¹äº†ä»€ä¹ˆ
git add -A        # å…¨éƒ¨åŠ å…¥æš‚å­˜åŒº
git commit -m "your message"
git push

```



### ä»£ç å¤‡å¿˜å½•









# å¤©å·¥ä»£ç 

## è°ƒè¯•æ€è·¯

**è°ƒè¯•~/Documents/TienKung-Lab/legged_lab/envs/base/base_config:**

**`rel_standing_envs: 0.2 â†’ 0.5`**
 æé«˜ç«™ç«‹/é›¶é€Ÿå‘½ä»¤å æ¯”ï¼Œä½¿è®­ç»ƒåˆ†å¸ƒç”±â€œä»¥èµ°ä¸ºä¸»â€è½¬ä¸ºâ€œèµ°åœè¿‘ä¹å„åŠâ€ï¼Œæ˜¾è‘—å¢å¼ºåœç¨³ã€å§¿æ€ä¿æŒä¸æŠ—æ‰°åŠ¨èƒ½åŠ›ï¼›ä»£ä»·æ˜¯è¡Œèµ°æ ·æœ¬ç›¸å¯¹å‡å°‘ï¼Œè¡Œèµ°æ€§èƒ½æ”¶æ•›å¯èƒ½ç•¥æ…¢ã€‚

**`resampling_time_range: (10,10) â†’ (9,10)`**
 ä¸ºå‘½ä»¤æŒç»­æ—¶é—´å¼•å…¥è½»å¾®éšæœºæ€§ï¼Œé¿å…å›ºå®šå‘¨æœŸå¯¼è‡´çš„æ—¶é—´ç›¸ä½è¿‡æ‹Ÿåˆï¼Œæå‡èµ°åœåˆ‡æ¢é²æ£’æ€§ä¸”ä¸æ˜¾è‘—å¢åŠ è®­ç»ƒä¸ç¨³å®šæ€§ã€‚

ç»“æœï¼š

2026-01-07_13-23-49 ï¼š 

    resampling_time_range: tuple = (6.0, 7.0)
    rel_standing_envs: float = 0.4

2026-01-07_18-41-48ï¼š

    resampling_time_range: tuple = (9.0, 10.0)
    rel_standing_envs: float = 0.5







**è°ƒè¯•~/Documents/TienKung-Lab/legged_lab/envs/tienkung/walk_cfg.py:**

```
stand_still_postureï¼š
```

- `func=mdp.stand_still`
- `weight=-0.02`
- `joint_names=[".*_joint"]`ï¼ˆå‡ ä¹æ‰€æœ‰å…³èŠ‚ï¼‰
- `zero_threshold=0.2`

è¿™æœ‰ä¸¤ä¸ªå…¸å‹é£é™©ï¼š

1. **è¦†ç›–å…³èŠ‚è¿‡å®½**ï¼šè¿æ‰‹è‡‚/ä¸Šèº«/é¢ˆéƒ¨ç­‰éƒ½è¢«å¼ºè¡Œæ‹‰å› defaultï¼Œå¯èƒ½å¯¼è‡´åœ¨è¡Œèµ°æ—¶ä¹Ÿè¢«â€œæ‹½ä½â€ï¼Œè®©æ­¥æ€å˜åƒµã€èƒ½é‡é£™å‡ã€tracking ä¸‹é™ã€‚
2. **æƒé‡å¯èƒ½åå¤§**ï¼ˆå–å†³äº `mdp.stand_still` çš„è¾“å‡ºå°ºåº¦ï¼‰ï¼šå¦‚æœå®ƒè¾“å‡ºæ˜¯â€œå¤šå…³èŠ‚ L1 æ±‚å’Œâ€ï¼Œé‚£æ•°å€¼å¯èƒ½æ˜¯ 5ï½å‡ åé‡çº§ï¼Œä¹˜ä»¥ -0.02 å¯èƒ½ç›´æ¥å˜æˆä¸€ä¸ª**å¤§è´Ÿé¡¹**ï¼ŒæŠŠå…¶ä»–å¥–åŠ±å‹æ‰ã€‚

ä¿®æ”¹ä¸€ï¼š

```python
stand_still_posture = RewTerm(
    func=mdp.stand_still,
    weight=-0.01,#åŸå§‹ä¸º-0.02   ä¿®æ­£æƒé‡å¯èƒ½åå¤§
    params={
        "asset_cfg": SceneEntityCfg(
            "robot",
            joint_names=[
                ".*_joint",
                ],
        ),
        "zero_threshold": 0.2,
        },
    )
```









**è°ƒè¯• åœ¨ä»¿çœŸä¸­ï¼Œæœºå™¨äººçš„è¿åŠ¨å‘½ä»¤éœ€è¦ç¡®å®šï¼š**

åŸå§‹çš„æ§åˆ¶æ–¹å¼å¯èƒ½æ˜¯playæ–‡ä»¶é‡Œçš„ï¼š

```
    env_cfg.commands.ranges.lin_vel_x = (1.0, 1.0)
    env_cfg.commands.ranges.lin_vel_y = (0.0, 0.0)
```



ç°åœ¨å·²ç»åœ¨playå’Œsimæ–‡ä»¶é‡ŒåŠ å…¥äº†è¿åŠ¨æ§åˆ¶çš„å‘½ä»¤ï¼š





**åªæ”¹ä¸€ä¸ªä¸œè¥¿è·‘ 2kï½5k iter çœ‹æ›²çº¿**ï¼ˆä¸è¦ä¸€æ¬¡æ”¹ 5 ä¸ª termï¼‰

åœ¨ TensorBoard é‡Œè®°å½•æ¯ä¸ª term çš„ï¼š

- mean(value)
- mean(weight*value)

åˆ¤æ–­é—®é¢˜å±äºå“ªç±»ï¼š

- Aï¼šè·Ÿè¸ªå·®ï¼ˆtracking ä½ï¼‰
- Bï¼šæŠ–åŠ¨å¤§ï¼ˆaction_rateã€acc ä¸å¤Ÿï¼‰
- Cï¼šè„šæ»‘ï¼ˆfeet_slide / æ‘©æ“¦ï¼‰
- Dï¼šåƒµç¡¬ï¼ˆposture/joint_deviation è¿‡å¼ºï¼‰

æŒ‰ä¼˜å…ˆçº§è°ƒï¼š

- ç«™ç«‹æŠ–ï¼šå…ˆåŠ¨ `action_rate`ï¼Œå†åŠ¨ `stand_still_posture`
- è¡Œèµ°è·Ÿè¸ªå·®ï¼šå…ˆå‡æƒ©ç½šå†åŠ  tracking
- èµ°åœåˆ‡æ¢ä¸ç¨³ï¼š`rel_standing_envs` + `stand_still`ï¼ˆä½†è¦é˜²è¯¯ä¼¤è¡Œèµ°ï¼‰



### 2026/1/13

walkå·²ç»æŒ‰ç…§å¸ˆå…„ç»™çš„ä»£ç è·‘äº†èµ·æ¥ã€‚

ç°åœ¨è¦è°ƒè¯•lite_run



## ä»£ç è°ƒç”¨å‘½ä»¤

### Tensorboard

**è°ƒå‡ºæœ€è¿‘çš„checkpointå’Œ tensorboard**

```bash
latest_run=$(ls -dt logs/*/* | head -n 1)
echo "LATEST RUN: $latest_run"

echo "TensorBoard events:"
find "$latest_run" -type f -name "events.out.tfevents.*"

echo "Checkpoints:"
find "$latest_run" -type f \( -name "*.pt" -o -name "*.pth" \)
```

**æŸ¥çœ‹TensorBoard**

```
tensorboard --logdir=logs  --port=6007
```



### Train

```
python legged_lab/scripts/train.py --task=walk --headless --logger=tensorboard --num_envs=4096

python legged_lab/scripts/train.py \
  --task=lite_run --headless --logger=tensorboard --num_envs=4096 \
  --log_name=lite_run-feet_near_2.2

```

```
python legged_lab/scripts/train.py \
  --task=pro_walk --headless --logger=tensorboard --num_envs=4096 \
  --log_name=pro_walk-original
```

```
CUDA_VISIBLE_DEVICES=1 PYTHONUNBUFFERED=1 \
python legged_lab/scripts/train.py \
  --task=pro_walk --headless --logger=tensorboard --num_envs=4096 \
  --log_name=pro_walk-original \
  --device=cuda:0 --info

```



### Play

```
python legged_lab/scripts/play.py --task=walk --num_envs=1
```

ä¿®æ”¹ /home/mig/Documents/TienKung-Lab/legged_lab/envs/tienkung/walk_cfg.py   æ–‡ä»¶ä¸­çš„ï¼šæ¥ä¿è¯ä½ ç°åœ¨playçš„åˆ°åº•æ˜¯å“ªä¸€ä¸ªpt

```python
	 resume = False

â€‹    \#load_run = ".*" #2026-01-05_19-55-14

â€‹    load_run = "2026-01-06_16-13-51" #2026-01-05_19-55-14

â€‹    \#load_checkpoint = "model_.*.pt"

â€‹    load_checkpoint = "model_14500.pt"

```



### Sim2Sim(MuJoCo)

å› ä¸ºsiméœ€è¦çš„ptæ–‡ä»¶å‡ºç°ä¸èƒ½ç”¨çš„æƒ…å†µï¼Œæˆ‘ä»trainå¯¼å‡ºçš„åŸå§‹ptä¸­åˆå¯¼äº†ä¸€ä¸‹ã€‚è°ƒç”¨ä¸‹è¿°ä»£ç ï¼š

```bash
  python export_jit_policy_from_ckpt.py \

â€‹    --ckpt logs/lite_walk/2026-01-11_15-47-58/model_13000.pt \

â€‹    --out  Exported_policy/walk_from_ckpt_13000.pt \

â€‹    --obs-dim 750 \

â€‹    --act-dim 20




```

è°ƒç”¨ä»¿çœŸæ¨¡å—ï¼šï¼ˆå¦‚æœåŸå§‹çš„walk.ptæ–‡ä»¶å¯ä»¥ç”¨ï¼‰ï¼š

```bash
python legged_lab/scripts/sim2sim.py --task walk --policy Exported_policy/walk.pt --duration 100
```

å¦‚æœä¸èƒ½ç”¨ï¼š

```bash
python legged_lab/scripts/sim2sim.py --task walk --policy Exported_policy/walk_from_ckpt_15000.pt --duration 100
 
python legged_lab/scripts/sim2sim.py --task walk --policy Exported_policy/walk_from_ckpt_6300.pt --duration 100 

```



## ä»£ç è°ƒè¯•è®°å½•1.9

##### 1ã€**è®­ç»ƒæ¨¡å‹ï¼š2026-01-09_13-46-39**

ä»¿çœŸè·¯å¾„ï¼š

```
python legged_lab/scripts/sim2sim.py --task walk --policy Exported_policy/walk_from_ckpt_6300.pt --duration 100 
```

**æ›´æ”¹å†…å®¹**

**rewards.py æ–‡ä»¶**

æ·»åŠ å¥–åŠ±é¡¹ï¼š

```python
def stand_still_exp(
    env: BaseEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    zero_threshold: float = 0.2,
) -> torch.Tensor:
    """Penalize joint deviation when command is near zero (standing)."""
    cmd = env.command_generator.command
    zero_flag = (
        (torch.linalg.norm(cmd[:, :2], dim=1) + torch.abs(cmd[:, 2])) <= zero_threshold
    ).to(cmd.dtype)  # [N] float

    asset: Articulation = env.scene[asset_cfg.name]
    angle = (
        asset.data.joint_pos[:, asset_cfg.joint_ids]
        - asset.data.default_joint_pos[:, asset_cfg.joint_ids]
    )
    return torch.exp(-20*torch.sum(torch.abs(angle), dim=1)) * zero_flag

```



**éšåä¿®æ”¹walk_cfg.pyæ–‡ä»¶ï¼š**

```python
    stand_still_exp = RewTerm(
    func=mdp.stand_still_exp,
    weight=5.0,  
    params={
        "asset_cfg": SceneEntityCfg(
            "robot",
           
            joint_names=[

                ".*_joint", 
            ],
        ),
        "zero_threshold": 0.2,
        },
    )
```

**ç»“æœ**

```
è¿˜æ˜¯åŸåœ°è¸æ­¥èµ°ï¼Œå¹¶ä¸”èµ·æ­¥é˜¶æ®µä¸å¹³ç¨³
```

**ä¸‹ä¸€æ­¥æ€è·¯**

```
1ã€åŠ å¤§stand_still_expçš„æ­£å‘æ¿€åŠ±å¢å¤§è‡³ 50 
2ã€åœ¨è®­ç»ƒçš„æ—¶å€™è¦æ˜ç¡®å‡ºæ¥ï¼Œè¡Œèµ°å’Œæš‚åœçš„ä¸åŒ
3ã€æ·»åŠ æ–°çš„åœæ­¢æ—¶å€™çš„rewars
```

##### 2ã€è®­ç»ƒæ¨¡å‹2026-01-09_15-26-06

**è°ƒæ•´äº†walk_cfg.pyæ–‡ä»¶ï¼š**

weightè°ƒæ•´åˆ°äº†50,ä½†æ˜¯æ²¡æœ‰ä»€ä¹ˆæ•ˆæœ

```python
    stand_still_exp = RewTerm(
    func=mdp.stand_still_exp,
    weight=50.0,  
    params={
        "asset_cfg": SceneEntityCfg(
            "robot",
           
            joint_names=[

                ".*_joint", 
            ],
        ),
        "zero_threshold": 0.2,
        },
    )
```

ä¸‹ä¸€æ­¥

```
å°è¯•åˆ«äººçš„ä»£ç ï¼Œå¹¶ææ¸…æ¥šé—®é¢˜å¤„åœ¨é‚£é‡Œ
```



##### ä½¿ç”¨CUDA 1å¡æ­»



ä½¿ç”¨å‘½ä»¤ï¼š

```
python legged_lab/scripts/train.py \train.py \
  --task=pro_walk --headless --logger=tensorboard --num_envs=4096 \
  --log_name=pro_walk-original \
  --device=cuda:1

```

å‡ºç°é—®é¢˜ï¼š

å¡æ­»ä¸åŠ¨

å› ä¸ºä¸Šé¢çš„å‘½ä»¤ä¸èƒ½å®Œå…¨åˆ‡æ¢åˆ°cuda 1

ä½¿ç”¨ä¸‹é¢çš„ï¼š

```
CUDA_VISIBLE_DEVICES=1 PYTHONUNBUFFERED=1 \
python legged_lab/scripts/train.py \
  --task=pro_walk --headless --logger=tensorboard --num_envs=4096 \
  --log_name=pro_walk-original \
  --device=cuda:0 --info

```

### 4ã€è°ƒè¯•pro_walk

#### 1æœˆ23

è°ƒè¯•æ¨¡å‹ï¼š**pro_walk-follow_lite_waist** æ·»åŠ äº† **waist_joint_deviation_l1**  æƒ©ç½šå‡½æ•°ï¼Œæƒé‡ä¸º-0.5

æ¨¡å‹è¡¨ç°ï¼šè®­ç»ƒæ›²çº¿**14K**åˆ°è¾¾900+ï¼Œç»´æŒä¸é•¿15.8Kå‡ºç°å´©æºƒï¼Œåé¢**17K**åˆé‡æ–°è®­ç»ƒèµ·æ¥ï¼Œè€Œæœ‰åˆå‡ºç°å´©æºƒ

å‡ºç°é—®é¢˜ï¼šæœºå™¨äººç«™ç«‹ä¸ç¨³ï¼Œæœºå™¨äººæ²¿Xè½´å’ŒZè½´æ–¹å‘æ™ƒåŠ¨

è§£å†³æ€è·¯ï¼š

| åŸå›                                                          | æ–¹æ¡ˆ                                                         |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| `Curriculum/terrain_levels` åœ¨ **~15K-16K** é™„è¿‘ä»ä½ç­‰çº§å¿«é€Ÿçˆ¬å‡åˆ° **levelâ‰ˆ5**ï¼Œéšååˆè¿…é€Ÿæ‰å›å»ï¼Œ                       è¿™æ˜¯å…¸å‹çš„ï¼šè¯¾ç¨‹éš¾åº¦çªç„¶æé«˜ â†’ å¤§é‡ç¯å¢ƒå¼€å§‹é¢‘ç¹ç»ˆæ­¢ï¼ˆè·Œå€’/è¶…é™/æ¥è§¦è§¦å‘ï¼‰â†’ on-policy æ•°æ®è´¨é‡æ€¥å‰§å˜å·® â†’ PPO ä¸€ä¸¤è½®æ›´æ–°å°±ä¼šâ€œæ‰“ç©¿â€å½“å‰ç­–ç•¥ç¨³å®šæ€§ â†’ æ›²çº¿åå¡Œã€‚ | æš‚æ—¶æŠŠ `max_init_terrain_level` ä» 5 é™åˆ° **2 æˆ– 3**ï¼›       |
| åœ¨â€œéœ€è¦ç”¨é«‹å…³èŠ‚åšå¹³è¡¡â€çš„æ—¶å€™ï¼Œé‡ç½šäº†é«‹å…³èŠ‚åŠ¨ä½œï¼Œ<br/>ç°åœ¨ï¼š `hip_roll_action weight = -2.0` `hip_yaw_action weight = -1.0`éå¸¸å®¹æ˜“å¯¼è‡´ï¼š<br/> **ç­–ç•¥ä¸æ•¢ç”¨é«‹ roll/yaw å»ä¿®æ­£ COM â†’ åªèƒ½ç”¨è¸/è†/è…°å»è¡¥å¿ â†’ å‡ºç°ä½é¢‘æ‘†åŠ¨ + è¿‡å†² â†’ X/Z æ™ƒåŠ¨ã€‚** | æŠŠ `hip_roll_action` ä» **-2.0 â†’ -0.5** <br/>`hip_yaw_action` ä» **-1.0 â†’  -0.3** |
|                                                              |                                                              |







### 5ã€è°ƒè¯•lite_run

#### 1.23

è°ƒè¯•æ¨¡å‹ï¼š**lite_run-2.0speed-1stand** åœ¨ è°ƒè¯•runçš„æ—¶å€™åªæ·»åŠ äº†ä¸€ä¸ªstand_stillçº¦æŸ---**stand_still_exp**

æ¨¡å‹è¡¨ç°ï¼š ç«™ç«‹ä¸ç¨³ï¼Œä½†æ˜¯è®­ç»ƒæ›²çº¿å¾ˆå¥½çœ‹ï¼Œstand_stillè¿˜åœ¨ç»§ç»­å¢é•¿ï¼Œç›®å‰è®­ç»ƒåˆ°**24.43K** å¯ä»¥ç»§ç»­









## å·¥ç¨‹æ¡†æ¶

æŠŠå®ƒæƒ³æˆå››å±‚ï¼š

**Layer Aï¼šä»¿çœŸåº•åº§ï¼ˆIsaac Sim / Isaac Labï¼‰**

- è´Ÿè´£ï¼šç‰©ç†ä»¿çœŸã€GPU PhysXã€èµ„äº§åŠ è½½ï¼ˆUSD articulationï¼‰ã€ä¼ æ„Ÿå™¨ã€å¹¶è¡Œç¯å¢ƒå¤åˆ¶

**Layer Bï¼šä»»åŠ¡ä¸ MDPï¼ˆlegged_labï¼‰**

- è´Ÿè´£ï¼šå®šä¹‰ä»»åŠ¡ï¼ˆwalkï¼‰ã€å®šä¹‰è§‚æµ‹ã€åŠ¨ä½œæ˜ å°„ã€å¥–åŠ±ï¼ˆmdp.*ï¼‰ã€ç»ˆæ­¢æ¡ä»¶ã€åŸŸéšæœºåŒ–ã€å‘½ä»¤ç”Ÿæˆå™¨
- ä½ å‘çš„ `LiteRewardCfg`ã€`TienKungWalkFlatEnvCfg` å°±åœ¨è¿™ä¸€å±‚

**Layer Cï¼šå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆrsl_rl / isaaclab_rl.rsl_rlï¼‰**

- è´Ÿè´£ï¼šPPOã€AMP PPOã€RNDã€å¯¹ç§°æ€§çº¦æŸã€rollout storageã€GAEã€ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨

**Layer Dï¼šè„šæœ¬ä¸æ³¨å†Œï¼ˆscripts + task_registryï¼‰**

- è´Ÿè´£ï¼šæŠŠ cfg + env class æ³¨å†Œåˆ° registryï¼Œæä¾› train.py è¿™æ ·çš„å…¥å£æŠŠæ‰€æœ‰ç»„ä»¶æ‹¼èµ·æ¥



1. `legged_lab/scripts/train.py`ï¼ˆä½ å·²ç»åœ¨çœ‹ï¼‰
2. `legged_lab/utils/task_registry.py`ï¼ˆæœ€å…³é”®ï¼šwalk å¦‚ä½•æ˜ å°„åˆ° cfg+classï¼‰
3. `legged_lab/envs/<tienkung>/...` é‡Œ walk å¯¹åº”çš„ env æ–‡ä»¶ï¼ˆçœŸæ­£çš„ `env_class`ï¼‰
4. `legged_lab/mdp/__init__.py` ä¸ reward å‡½æ•°æ–‡ä»¶ï¼ˆä½ å·²ç»è´´è¿‡ï¼‰
5. `rsl_rl/runners/amp_on_policy_runner.py`ï¼ˆlearn å¾ªç¯ä¸ AMP å¦‚ä½•æ··åˆ rewardï¼‰
6. `rsl_rl/algorithms/amp_ppo.py`ï¼ˆAMPPPO çš„ loss å…¬å¼ä¸è®­ç»ƒæ­¥éª¤ï¼‰

è¿™æ ·ä½ ä¼šéå¸¸å¿«åœ°æŠŠâ€œå…¥å£ â†’ æ³¨å†Œ â†’ ç¯å¢ƒ â†’ reward â†’ runner â†’ algorithmâ€ä¸²èµ·æ¥ã€‚

### ç³»ç»Ÿé—­ç¯

**ä½ è·‘ `python train.py --task=walk ...` åï¼Œç³»ç»Ÿå‘ç”Ÿçš„ 3 ä¸ªé—­ç¯**

**é—­ç¯ 1ï¼šæ§åˆ¶é—­ç¯ï¼ˆPolicy â†’ Environment â†’ Next Obsï¼‰**

1. policy æ ¹æ® obs + command è¾“å‡º action
2. env.step(action) åœ¨ Isaac Sim é‡Œæ¨è¿›è‹¥å¹²ç‰©ç†æ­¥ï¼ˆdecimationï¼‰
3. env è¿”å› next_obsã€rewardã€doneã€infoï¼ˆå¹¶è¡Œ N ä¸ª envï¼‰

**é—­ç¯ 2ï¼šå­¦ä¹ é—­ç¯ï¼ˆRollout â†’ GAE â†’ PPO æ›´æ–°ï¼‰**

1. é‡‡æ ·ä¸€æ®µ rolloutï¼ˆnum_steps_per_envï¼‰
2. ç”¨ critic ä¼°è®¡ valueï¼Œç®—ä¼˜åŠ¿ A_tï¼ˆGAEï¼‰ä¸ returns
3. PPO å¤š epochã€å¤š mini-batch æ›´æ–° actor/critic

**é—­ç¯ 3ï¼šAMP é—­ç¯ï¼ˆExpert vs Policy â†’ Discriminator â†’ AMP rewardï¼‰**

1. ä» expert motion æ–‡ä»¶é‡‡æ ·â€œä¸“å®¶ç‰‡æ®µâ€ï¼ˆexpert transitionsï¼‰
2. ä»å½“å‰ policy rollout æŠ½â€œç­–ç•¥ç‰‡æ®µâ€ï¼ˆpolicy transitionsï¼‰
3. è®­ç»ƒåˆ¤åˆ«å™¨åŒºåˆ† expert/policy
4. ç”¨åˆ¤åˆ«å™¨è¾“å‡ºæ„é€  AMP rewardï¼Œæ··åˆ°æ€» reward é‡Œåå‘å¡‘å½¢ policy

------

#### 1) `legged_lab/scripts/train.py`

å®ƒä¸æ‡‚â€œèµ°è·¯ç»†èŠ‚â€ï¼Œåªåšæ‹¼è£…ä¸å¯åŠ¨ï¼š

- è§£æå‚æ•°ï¼š`--task=walk --num_envs=64 --headless --logger=tensorboard`
- å¯åŠ¨ Isaac Simï¼š`AppLauncher`
- è§¦å‘ä»»åŠ¡æ³¨å†Œï¼š`from legged_lab.envs import *`
- ä» registry æ‹¿ä¸‰æ ·ä¸œè¥¿ï¼š`env_cfg, agent_cfg, env_class`
- ç”¨ cfg åˆ›å»º envï¼Œç”¨ agent_cfg åˆ›å»º runner
- `runner.learn()` è¿›å…¥è®­ç»ƒä¸»å¾ªç¯

> train.py æ˜¯â€œæ€»æ§å¼€å…³â€ï¼Œä¸åŒ…å«ç®—æ³•ç»†èŠ‚ï¼Œä¹Ÿä¸åŒ…å«ä»»åŠ¡ç»†èŠ‚ã€‚

------

#### 2) `legged_lab/utils/task_registry.py`

**ï¼šä»»åŠ¡è·¯ç”±è¡¨ï¼ˆå­—ç¬¦ä¸² â†’ ç±»ä¸é…ç½®ï¼‰**

è¿™æ˜¯ä¸€ä¸ªå…¨å±€å­—å…¸ï¼š

- `"walk"` â†’ `env_class`ï¼ˆå“ªä¸ªç¯å¢ƒç±»ï¼‰
- `"walk"` â†’ `env_cfg`ï¼ˆç¯å¢ƒé…ç½®å¯¹è±¡ï¼‰
- `"walk"` â†’ `agent_cfg`ï¼ˆè®­ç»ƒé…ç½®å¯¹è±¡ï¼‰

å®ƒçš„ä»·å€¼æ˜¯è§£è€¦ï¼štrain.py æ°¸è¿œä¸éœ€è¦å†™æ­» `TienKungWalkFlatEnvCfg` è¿™ç§ç±»åï¼Œåªè¦ä¼ ä¸€ä¸ªå­—ç¬¦ä¸²å°±è¡Œã€‚

> æ³¨å†Œé€šå¸¸å‘ç”Ÿåœ¨ `legged_lab/envs/__init__.py` æˆ–å­æ¨¡å— import çš„ side-effect ä¸­ã€‚

------

#### 3) `legged_lab/envs/<tienkung>/...`

**ï¼šçœŸæ­£çš„ç¯å¢ƒå®ç°ï¼ˆMDP çš„â€œè¿è¡Œæ—¶å¼•æ“â€ï¼‰**

è¿™æ˜¯é¡¹ç›®â€œä»»åŠ¡å±‚â€çš„æ ¸å¿ƒï¼šæŠŠ cfg å˜æˆä¸€ä¸ªå¯äº¤äº’çš„ `VecEnv`ã€‚

å®è§‚ä¸Šï¼Œè¿™ä¸ªç¯å¢ƒç±»ä¼šå®ç°ï¼ˆæˆ–ç»„åˆå®ç°ï¼‰ï¼š

- **åœºæ™¯æ„å»º**ï¼šåŠ è½½æœºå™¨äºº articulationã€åœ°å½¢ã€ä¼ æ„Ÿå™¨ï¼ˆcontactã€height scanner ç­‰ï¼‰
- **çŠ¶æ€ä¸ç¼“å­˜**ï¼šobs historyã€action bufferã€episode buffersï¼ˆreset_buf/time_out_bufï¼‰
- **å‘½ä»¤ç”Ÿæˆ**ï¼šcommand_generator é‡‡æ ·é€Ÿåº¦/èˆªå‘æŒ‡ä»¤ï¼ˆå«ç«™ç«‹æ¯”ä¾‹ï¼‰
- **å¥–åŠ±ç®¡ç†**ï¼šæŠŠ `LiteRewardCfg` é‡Œæ¯ä¸ª term ç¼–è¯‘æˆå¯è®¡ç®—é¡¹ï¼ˆè°ƒç”¨ mdp.reward å‡½æ•°ï¼‰
- **ç»ˆæ­¢ä¸é‡ç½®**ï¼šæ¥è§¦ç»ˆæ­¢ã€è¶…æ—¶ç»ˆæ­¢ï¼›reset æ—¶æ‰§è¡Œ domain rand events
- **step æµç¨‹**ï¼šaction â†’ ç‰©ç†æ¨è¿›ï¼ˆdecimationï¼‰â†’ æ›´æ–°ä¼ æ„Ÿå™¨/çŠ¶æ€ â†’ obs/reward/done

ä½ å¯ä»¥æŠŠ env ç±»çœ‹æˆï¼š**MDP å®šä¹‰ + Isaac Sim æ‰§è¡Œ** çš„æ¡¥æ¢ã€‚

------

#### 4) legged_lab/mdp/ reward

 **æ–‡ä»¶ï¼šMDP ä¸­çš„â€œæ•°å­¦éƒ¨åˆ†â€**

è¿™ä¸€å±‚é€šå¸¸æ˜¯â€œçº¯å‡½æ•°å¼â€çš„ï¼šç»™å®š env çš„çŠ¶æ€å¼ é‡ï¼Œè®¡ç®—æŸä¸ª reward/penaltyã€‚

ä¾‹å¦‚ï¼š

- è·Ÿè¸ªé€Ÿåº¦ï¼š`exp(-||v_cmd - v||^2 / std^2)`
- å§¿æ€å¹³è¡¡ï¼šroll/pitch ç›¸å…³æƒ©ç½š
- æ¥è§¦çº¦æŸï¼šundesired contactsã€feet slideã€feet stumble
- åŠ¨ä½œå¹³æ»‘ï¼šaction_rateã€joint_acc
- ç«™ç«‹/è¡Œèµ°é—¨æ§ï¼š`zero_flag = ||cmd|| < eps`ï¼ˆå®ç°è¡Œä¸ºæ¨¡å¼åˆ‡æ¢ï¼‰
- gait clockï¼šç›¸ä½ maskï¼ˆswing/stanceï¼‰å¡‘å½¢æ­¥æ€

å®è§‚é‡è¦ç‚¹ï¼š
 **reward æ–‡ä»¶ä¸æ˜¯â€œç¯å¢ƒæ‰§è¡Œâ€ï¼Œè€Œæ˜¯â€œç›®æ ‡å‡½æ•°å®šä¹‰â€**ã€‚å®ƒå†³å®šç­–ç•¥æœ€ç»ˆå­¦æˆä»€ä¹ˆæ ·ã€‚

------

#### 5) `rsl_rl/runners/amp_on_policy_runner.py`

**è®­ç»ƒå¾ªç¯çš„å‘åŠ¨æœºï¼ˆé‡‡æ · + æ›´æ–° + æ—¥å¿—ï¼‰**

Runner è´Ÿè´£æŠŠ env å’Œç®—æ³•è¿èµ·æ¥ï¼Œæ ¸å¿ƒèŒè´£æ˜¯â€œæŠŠé—­ç¯è·‘èµ·æ¥â€ï¼š

- **Rollout**ï¼šå¾ªç¯ num_steps_per_env æ¬¡
  - è°ƒ actor å¾— action
  - env.step å¾— obs/reward/done
  - å­˜åˆ° rollout bufferï¼šobs, actions, rewards, dones, values, log_probs
- **Compute returns/advantages**ï¼šGAEï¼ˆgamma, lamï¼‰
- **ä¼˜åŒ–**ï¼šè°ƒç”¨ algorithm çš„ updateï¼ˆPPO/AMPPPOï¼‰
- **AMP æ‰©å±•ï¼ˆAmpOnPolicyRunner ç‰¹æœ‰ï¼‰**
  - åŒæ—¶ç»´æŠ¤ expert buffer / policy buffer
  - è®­ç»ƒ discriminator
  - è®¡ç®— AMP reward å¹¶æ··åˆåˆ° task rewardï¼ˆå— `amp_task_reward_lerp`ã€`amp_reward_coef` ç­‰æ§åˆ¶ï¼‰
- **æ—¥å¿—/ä¿å­˜**ï¼štensorboardã€checkpoint

ä½ å¯ä»¥æŠŠ runner ç†è§£ä¸ºï¼š**è®­ç»ƒç®¡çº¿çš„æµæ°´çº¿æ§åˆ¶å™¨**ã€‚

------

#### 6) `rsl_rl/algorithms/amp_ppo.py`

**AMPPPO çš„â€œæ•°å­¦æ›´æ–°è§„åˆ™â€**

è¿™æ˜¯æœ€â€œç®—æ³•å±‚â€çš„ä¸œè¥¿ï¼šå®šä¹‰ lossã€åå‘ä¼ æ’­ã€æ¢¯åº¦æ›´æ–°ã€‚

å®è§‚ç»„æˆé€šå¸¸æ˜¯ä¸‰å—ï¼ˆæ¯æ¬¡ updateï¼‰ï¼š

**6.1 PPO actor lossï¼ˆclipï¼‰**

- ratio = exp(new_logp - old_logp)
- clip ratio åˆ° [1-Îµ, 1+Îµ]
- å– min/æˆ– max å½¢æˆ clipped objective
- ç”¨ advantage åšæƒé‡

**6.2 PPO critic lossï¼ˆvalueï¼‰**

- MSE(return - value)
- å¸¸è§è¿˜æœ‰ clipped value lossï¼ˆä½  cfg é‡Œæ‰“å¼€äº†ï¼‰

**6.3 AMP discriminator loss + AMP reward**

- discriminator å­¦ä¼šåŒºåˆ† expert vs policyï¼ˆç±»ä¼¼ GAN çš„åˆ¤åˆ«å™¨ï¼‰
- ç”¨åˆ¤åˆ«å™¨è¾“å‡ºæ„é€ ä¸€ä¸ªâ€œåƒä¸“å®¶å°±é«˜åˆ†â€çš„ reward
- æœ€ç»ˆæ€» reward è¿›å…¥ advantage/returnï¼Œä»è€Œå½±å“ actor/critic æ›´æ–°

> è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ AMP èƒ½è®©åŠ¨ä½œâ€œæ›´åƒäººèµ°è·¯â€ï¼šå®ƒæŠŠâ€œåƒä¸“å®¶â€å˜æˆä¼˜åŒ–ç›®æ ‡çš„ä¸€éƒ¨åˆ†ã€‚

------

**æŠŠ 6 ä¸ªæ¨¡å—è¿æˆä¸€æ¡â€œæ¸…æ™°çš„å› æœé“¾â€**

ä½ å¯ä»¥è®°ä½è¿™æ¡é“¾ï¼š

**train.py**ï¼ˆå¯åŠ¨/æ‹¼è£…ï¼‰
 â†’ **task_registry**ï¼ˆæŠŠå­—ç¬¦ä¸²æ˜ å°„åˆ° env_class + cfgï¼‰
 â†’ **env_class**ï¼ˆæŠŠ cfg å˜æˆå¯ step çš„å¹¶è¡Œä»¿çœŸç¯å¢ƒï¼‰
 â†’ **mdp.reward**ï¼ˆenv åœ¨æ¯æ­¥è®¡ç®— reward term çš„æ•°å­¦å®šä¹‰ï¼‰
 â†’ **AmpOnPolicyRunner**ï¼ˆrollout + GAE + è°ƒç”¨ç®—æ³•æ›´æ–° + è®°å½•æ—¥å¿—ï¼‰
 â†’ **AMPPPO**ï¼ˆPPO loss + AMP åˆ¤åˆ«å™¨ lossï¼Œåšæ¢¯åº¦ä¸‹é™ï¼‰
 â†’ æ›´æ–°åçš„ policy å›åˆ° env å†é‡‡æ ·ï¼ˆå¾ªç¯ï¼‰

------

**ä½ ç°åœ¨æœ€è¯¥å…ˆæŠ“ä½çš„ 3 ä¸ªâ€œå®è§‚æŠ“æ‰‹â€**

1. **æ•°æ®ç»“æ„æŠ“æ‰‹**ï¼šrunner ä¸ env äº¤äº’çš„æœ€å…³é”®å››å…ƒç»„
   - obsï¼ˆpolicy è¾“å…¥ï¼‰
   - actionï¼ˆpolicy è¾“å‡ºï¼‰
   - rewardï¼ˆä¼˜åŒ–ç›®æ ‡ï¼‰
   - done/resetï¼ˆæ•°æ®åˆ†æ®µä¸ç»ˆæ­¢ï¼‰
2. **ä¸¤ç±»å¥–åŠ±æŠ“æ‰‹**ï¼štask reward vs AMP reward
   - task rewardï¼šè·Ÿè¸ªé€Ÿåº¦ã€ç¨³å®šã€ä¸æ‘”
   - AMP rewardï¼šåƒä¸“å®¶èµ°è·¯ï¼ˆé£æ ¼/è‡ªç„¶æ€§ï¼‰
3. **é—¨æ§æŠ“æ‰‹**ï¼šcommand å†³å®šâ€œç«™ç«‹ vs è¡Œèµ°â€çš„æ¨¡å¼åˆ‡æ¢
   - å¾ˆå¤š reward term ä¼šç”¨ `||cmd|| < eps` ä½œä¸ºå¼€å…³
   - æ‰€ä»¥ä¿®æ”¹ reward å°±èƒ½æ”¹å˜åˆ‡æ¢è¡Œä¸º





#### å¤©å·¥TensorBoard  Rewardå‚æ•°

##### **1ã€Action_rate_l2**

è¡¡é‡ **ç›¸é‚»ä¸¤ä¸ª timestep ä¹‹é—´ action å˜åŒ–çš„å¹³æ–¹å’Œ**ï¼š

![image-20251231110032553](/home/mig/Desktop/typora_images/Miniconda3-latest-Linux-x86_64.sh)

æƒ©ç½š **é«˜é¢‘æŠ–åŠ¨**

é¼“åŠ±åŠ¨ä½œå¹³æ»‘

é˜²æ­¢æ§åˆ¶ä¿¡å·â€œéœ‡è¡â€

##### **2ã€Ang_vel_xy_l2**

æ§åˆ¶èº«ä½“è§’é€Ÿåº¦ï¼ˆpitch/rollï¼‰æ¥è¿‘ç›®æ ‡ï¼ˆé€šå¸¸æ˜¯ 0ï¼‰ï¼š

![image-20251231110317710](/home/mig/snap/typora/110/.config/Typora/typora-user-images/image-20251231110317710.png)

ä¿è¯èº«ä½“ä¸æ‘‡ã€ä¸ç¿»

æ˜¯â€œå§¿æ€ç¨³å®šæ€§â€çš„æ ¸å¿ƒæŒ‡æ ‡

##### **3ã€Ankle_action & Ankle_torque**

**ï¼ˆè¸å…³èŠ‚åŠ¨ä½œä¸åŠ›çŸ©æƒ©ç½šï¼‰**

`ankle_action`ï¼šè¸å…³èŠ‚æ§åˆ¶è¾“å…¥å¹…åº¦

`ankle_torque`ï¼šå®é™…è¾“å‡ºåŠ›çŸ©

é˜²æ­¢è¸å…³èŠ‚å‰§çƒˆæ‘†åŠ¨

é¿å… unrealistic torque

##### 4ã€Body_orientation_l2

ï¼ˆèº«ä½“å§¿æ€åå·®ï¼‰

èº¯å¹²å§¿æ€ç›¸å¯¹ upright çš„åå·®å¹³æ–¹

é˜²æ­¢å‰å€¾/åä»°/ä¾§ç¿»

##### 5ã€Dof_acc_l2 & Dof_pos_limits

ï¼ˆå…³èŠ‚åŠ é€Ÿåº¦ & å…³èŠ‚æé™ï¼‰

`dof_acc_l2`ï¼šå…³èŠ‚åŠ é€Ÿåº¦å¹³æ»‘æ€§

`dof_pos_limits`ï¼šæ˜¯å¦æ¥è¿‘æœºæ¢°æé™



##### 6ã€Energy

åŠŸç‡æˆ–åŠ›çŸ©èƒ½è€—æƒ©ç½š

é¼“åŠ±èŠ‚èƒ½æ­¥æ€

##### 7ã€Feet_too_near

åŒè„šé—´è·è¿‡è¿‘çš„æƒ©ç½š

é˜²æ­¢ç»Šè„šã€äº¤å‰

##### 8ã€Feet_slide

è¶³åº•æ³•å‘åŠ›

æ°´å¹³æ»‘ç§»

##### 9ã€ Feet_stumble

è„šåœ¨éé¢„æœŸçŠ¶æ€ä¸‹ç¢°æ’æˆ–ç»Šå€’



#### å¤©å·¥TensorBoard  Losså‚æ•°

#####  Loss / amp

AMPï¼ˆAdversarial Motion Priorï¼‰é€šå¸¸æŠŠâ€œè®©ç­–ç•¥åŠ¨ä½œåƒ expertâ€çš„ç›®æ ‡å†™æˆä¸€ä¸ªå¯¹æŠ—å­¦ä¹ é—®é¢˜ï¼š

- ä¸€ä¸ª **åˆ¤åˆ«å™¨ D(s)** åŒºåˆ†â€œexpert è½¨è¿¹â€ä¸â€œpolicy è½¨è¿¹â€
- policy é€šè¿‡ AMP rewardï¼ˆæ¥è‡ª Dï¼‰è¢«é©±åŠ¨å»â€œéª—è¿‡åˆ¤åˆ«å™¨â€

`Loss/amp` ä¸€èˆ¬å°±æ˜¯ AMP åˆ¤åˆ«å™¨è®­ç»ƒçš„æ€» lossï¼ˆæˆ–å…¶ç›‘æ§æŒ‡æ ‡ï¼‰ï¼Œå¸¸ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š

![image-20251231112338575](/home/mig/snap/typora/110/.config/Typora/typora-user-images/image-20251231112338575.png)

å®ƒåæ˜  **åˆ¤åˆ«å™¨æ˜¯å¦åœ¨å­¦ä¹ **ã€æ˜¯å¦å·²ç»é¥±å’Œã€æ˜¯å¦å‡ºç°å´©åï¼ˆè¿‡å¼º/è¿‡å¼±ï¼‰ã€‚

**å…¸å‹å½¢æ€æ€ä¹ˆè¯»**

- **ä¸‹é™å¹¶è¶‹ç¨³**ï¼šåˆ¤åˆ«å™¨å­¦åˆ°äº†è¾ƒç¨³å®šçš„åŒºåˆ†è¾¹ç•Œï¼ˆæ­£å¸¸ï¼‰
- **é•¿æœŸæŒ¯è¡ä½†ä¸å‘æ•£**ï¼šå¯¹æŠ—è¾¾åˆ°åŠ¨æ€å¹³è¡¡ï¼ˆæ­£å¸¸/å¯æ¥å—ï¼‰
- **çªç„¶é£™å‡æˆ–å½’é›¶**ï¼šåˆ¤åˆ«å™¨è®­ç»ƒå‡ºé—®é¢˜ï¼ˆæ•°æ®ã€å­¦ä¹ ç‡ã€æ¢¯åº¦æƒ©ç½šã€å½’ä¸€åŒ–ç­‰ï¼‰

ä½ å½“å‰å›¾é‡Œ `amp` å¤§è‡´ç¨³å®šåœ¨ä¸€ä¸ªèŒƒå›´å†…å¹¶æŒ¯è¡ï¼Œé€šå¸¸æ„å‘³ç€ï¼š

> åˆ¤åˆ«å™¨è®­ç»ƒç¨³å®šï¼Œä½†å¯¹ policy æä¾›çš„â€œå¯ç”¨å­¦ä¹ ä¿¡å·â€å¯èƒ½å·²ç»å˜å¼±ï¼ˆé¥±å’Œï¼‰ã€‚





##### Loss / amp_expert_pred

è¿™æ˜¯â€œåˆ¤åˆ«å™¨å¯¹ expert æ ·æœ¬çš„è¾“å‡ºï¼ˆæˆ–å¯¹åº”æŸå¤±ï¼‰â€çš„ç›‘æ§ã€‚
 ä¸åŒå®ç°å¯èƒ½è®°å½•ï¼š

- **D(expert) çš„å¹³å‡è¾“å‡º**ï¼ˆè¶Šæ¥è¿‘â€œexpertâ€è¶Šå¥½ï¼‰
- æˆ– **expert åˆ†ç±»æŸå¤±**ï¼ˆè¶Šä½è¶Šå¥½ï¼‰

ä½ å†™çš„â€œExpert æ•°æ®è¢«åˆ¤åˆ«ä¸º expert çš„ç½®ä¿¡åº¦â€å±äºç¬¬ä¸€ç§è§£é‡Šã€‚

å®ƒå‘Šè¯‰ä½ ï¼š

- åˆ¤åˆ«å™¨æ˜¯å¦èƒ½æŠŠ expert çœ‹æˆ expertï¼ˆå¦åˆ™åˆ¤åˆ«å™¨åäº†ï¼‰
- expert æ•°æ®é¢„å¤„ç†/å½’ä¸€åŒ–æ˜¯å¦å¯¹é½ policy æ•°æ®ï¼ˆå¦åˆ™åˆ†å¸ƒæ¼‚ç§»ï¼‰

- **D(expert) å¾ˆé«˜ä¸”ç¨³å®š**ï¼šåˆ¤åˆ«å™¨è‡³å°‘èƒ½è¯†åˆ« expertï¼ˆæ­£å¸¸ï¼‰
- **D(expert) é€æ¸å˜å·®**ï¼šåˆ¤åˆ«å™¨è®­ç»ƒä¸ç¨³æˆ– expert æ•°æ®è¾“å…¥æ ¼å¼æœ‰é—®é¢˜
- **D(expert) è¿‡é¥±å’Œï¼ˆé•¿æœŸæç«¯å€¼ï¼‰**ï¼šåˆ¤åˆ«å™¨å¯èƒ½è¿‡å¼ºï¼Œå¯¼è‡´ policy æ¢¯åº¦â€œæ— ä¿¡æ¯â€



#####  Amp_grad_pen

WGAN-GP é£æ ¼çš„æ¢¯åº¦æƒ©ç½šé¡¹ï¼Œé˜²æ­¢åˆ¤åˆ«å™¨å˜å¾—è¿‡å°–é”å¯¼è‡´ä¸ç¨³å®šï¼š

å®ƒæ˜¯ AMP ç¨³å®šæ€§çš„â€œä¿é™©ä¸â€ã€‚

- å¤ªå°ï¼šåˆ¤åˆ«å™¨å¯èƒ½è¿‡åº¦å°–é”ï¼Œè®­ç»ƒä¼šæŠ–/å´©
- å¤ªå¤§ï¼šåˆ¤åˆ«å™¨è¢«å¼ºè¡Œå‹å¹³ï¼Œå­¦ä¹ èƒ½åŠ›ä¸è¶³ï¼Œpolicy å¾—ä¸åˆ°æœ‰æ•ˆä¿¡å·

æ€ä¹ˆè¯»

- ç¨³å®šåœ¨ä¸­ç­‰æ°´å¹³ï¼šé€šå¸¸æ­£å¸¸
- æŒç»­å¢å¤§ï¼šåˆ¤åˆ«å™¨æ¢¯åº¦åœ¨å˜â€œé‡â€ï¼Œå¯èƒ½å­¦ä¹ ç‡åå¤§ã€è¾“å…¥æœªå½’ä¸€åŒ–æˆ– reward/obs åˆ†å¸ƒæ¼‚ç§»
- æ¥è¿‘ 0 ä¸” amp_expert_pred/amp_policy_pred ä¹Ÿé¥±å’Œï¼šåˆ¤åˆ«å™¨å¯èƒ½è¢«â€œå‹å¹³ + é¥±å’Œâ€ï¼Œç»™ä¸å‡ºä¿¡æ¯





##### **Amp_policy_pred**

ç­–ç•¥æ ·æœ¬è¢«åˆ¤åˆ«æˆ expert çš„æ¦‚ç‡ï¼ˆè¶Šæ¥è¿‘ expert è¶Šå¥½ï¼‰

åˆ¤åˆ«å™¨å¯¹ policy æ ·æœ¬çš„è¾“å‡ºï¼ˆæˆ–å…¶æŸå¤±ï¼‰ã€‚

- **D(policy) è¶Šé«˜** â‡’ policy è¶Šåƒ expert

è¿™æ˜¯ AMP å¯¹ policy çš„æ ¸å¿ƒåé¦ˆï¼š

- å¦‚æœ policy è¶Šæ¥è¶Šåƒ expertï¼Œè¿™ä¸ªæŒ‡æ ‡åº”é€æ­¥â€œå‘ expert æ–¹å‘ç§»åŠ¨â€
- å¦‚æœé•¿æœŸä¸åŠ¨ï¼Œè¯´æ˜ AMP reward å¯¹ policy å·²ç»æ²¡æœ‰é©±åŠ¨åŠ›ï¼ˆæˆ–è¢«å…¶ä»–æƒ©ç½šé¡¹å‹ä½ï¼‰

- **æŒç»­æ”¹å–„å¹¶è¶‹ç¨³**ï¼šAMP åœ¨å‘æŒ¥ä½œç”¨

- **æ—©æœŸæ”¹å–„ï¼ŒåæœŸå¹³å°**ï¼šAMP ä¿¡å·é¥±å’Œ/è¢« PPO ç¨³å®šé¡¹é”æ­»

- **åæœŸåå‘æ¶åŒ–**ï¼špolicy è¢« task reward/penalty æ‹‰åï¼ŒAMP æ‹‰ä¸å›æ¥ï¼ˆå¸¸è§äºä½ è¿™ç§ reward æƒ©ç½šè¿‡å¼ºï¼‰

  

**Entropy** 

ç­–ç•¥åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§ã€‚å¯¹é«˜æ–¯ç­–ç•¥ï¼ˆè¿ç»­åŠ¨ä½œï¼‰ï¼š

![image-20251231113531254](/home/mig/snap/typora/110/.config/Typora/typora-user-images/image-20251231113531254.png)

å¾ˆå¤š logger è®°å½•çš„æ˜¯ entropy æœ¬èº«ï¼Œä¹Ÿå¯èƒ½è®°å½• `-entropy`ï¼ˆçœ‹å®ç°ï¼‰ã€‚
 ä½ å›¾é‡Œä»é«˜åˆ°ä½å¿«é€Ÿä¸‹é™ï¼Œå¹¶åœ¨è¾ƒä½å€¼å¹³å°ï¼Œç¬¦åˆâ€œæ¢ç´¢å¿«é€Ÿæ¶ˆå¤±â€ã€‚

entropy æ˜¯ PPO çš„â€œæ¢ç´¢æ¸©åº¦è®¡â€ï¼š

- **å¤ªé«˜**ï¼šç­–ç•¥å¤ªéšæœºï¼Œå­¦ä¸ç¨³

- **å¤ªä½**ï¼šç­–ç•¥è¿‡æ—©ç¡®å®šï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜å¹¶åœæ­¢æ”¹è¿›ï¼ˆä½ ç°åœ¨çš„å…¸å‹çŠ¶æ€ï¼‰

  

- åˆæœŸä¸‹é™ï¼šæ­£å¸¸ï¼ˆä»éšæœºæ¢ç´¢èµ°å‘æ”¶æ•›ï¼‰
- ä¸‹é™åä»ç¼“æ…¢å˜åŒ–ï¼šå¥åº·ï¼ˆè¿˜èƒ½ç»§ç»­å¾®è°ƒï¼‰
- **è¿‡æ—©é™åˆ°å¾ˆä½ä¸”é•¿æœŸå¹³**ï¼šç­–ç•¥å†»ç»“ï¼ˆPPO æ›´æ–°å¹…åº¦å°ã€KL å°ã€ä¼˜åŠ¿å°ã€lr å°ï¼‰



#####  Learning_rate

ä¼˜åŒ–å™¨å½“å‰ LRï¼ˆå¯èƒ½æ¥è‡ª scheduleï¼‰ã€‚

LR å†³å®š PPO æ›´æ–°â€œèƒ½ä¸èƒ½åŠ¨èµ·æ¥â€ã€‚å¦‚æœä½ è¿™é‡Œå‡ ä¹æ‰åˆ° 0ï¼š

> é‚£ä¹ˆåé¢å³ä½¿ reward æœ‰ä¿¡å·ï¼Œç½‘ç»œå‚æ•°ä¹ŸåŸºæœ¬ä¸ä¼šæ›´æ–°ã€‚

- `constant` æˆ–ç¼“æ…¢ decayï¼šå¸¸è§
- **å¾ˆå¿«è¡°å‡åˆ°æ¥è¿‘ 0**ï¼šå¸¸è§è¯¯é…ç½®ï¼ˆscheduler å¤ªæ¿€è¿›ã€æ€»æ­¥æ•°è®¾é”™ã€warmup/anneal é€»è¾‘æœ‰ bugï¼‰

å·¥ç¨‹ä¸Šï¼Œå¦‚æœä½ æƒ³ç»§ç»­å­¦ä¹ ï¼Œé€šå¸¸åº”è¯¥è®¾ç½®ï¼š

- LR ä¸‹é™ï¼ˆfloorï¼‰ï¼Œä¾‹å¦‚ `min_lr=3e-5`

- æˆ–ç›´æ¥ constant LRï¼ˆè‡³å°‘åœ¨æ—©æœŸï¼‰

  

##### Surrogate

PPO çš„æ ¸å¿ƒç›®æ ‡ï¼ˆclipped surrogate objectiveï¼‰ï¼Œä¸€èˆ¬ç›‘æ§çš„æ˜¯ **è´Ÿçš„ç›®æ ‡å€¼** æˆ–å…¶å‡å€¼ï¼š

logger è‹¥è®°å½•ä¸º `loss/surrogate`ï¼Œå¸¸è§æ˜¯æŠŠè¦æœ€å¤§åŒ–çš„ç›®æ ‡å–è´Ÿå·åå˜æˆâ€œlossâ€ã€‚

**ä¸ºä»€ä¹ˆé‡è¦**

å®ƒç›´æ¥åæ˜ ï¼š

- ä¼˜åŠ¿ $A_t$ æ˜¯å¦æœ‰ä¿¡å·
- policy æ›´æ–°æ˜¯å¦åœ¨å‘ç”Ÿ
- clip æ˜¯å¦é¢‘ç¹è§¦å‘ï¼ˆæ›´æ–°å¤ªå¤§/å¤ªå°ï¼‰

**æ€ä¹ˆè¯»**

- æœ‰æ³¢åŠ¨ä¸”æœ‰è¶‹åŠ¿ï¼šåœ¨å­¦ä¹ 
- **é•¿æœŸç¨³å®šåœ¨ä¸€ä¸ªå°å¹…åŒºé—´**ï¼špolicy æ›´æ–°å¾ˆå°ã€ä¼˜åŠ¿å¾ˆå°ã€æˆ– LR å¤ªå°/entropy å¤ªä½å¯¼è‡´å†»ç»“
- å¦‚æœä½ åŒæ—¶çœ‹åˆ° entropy ä½ã€lr ä½ï¼Œé‚£ surrogate ç¨³å®šåŸºæœ¬å°±æ˜¯â€œæ²¡åœ¨å­¦â€





**Value_function**



ä»·å€¼å‡½æ•°æ‹Ÿåˆè¯¯å·®ï¼ˆMSE/Huberï¼‰ï¼š
$$
L_V = \mathbb{E}\big[(V_\phi(s_t) - \hat{R}_t)^2\big]
$$


**ä¸ºä»€ä¹ˆé‡è¦**

critic è´¨é‡å†³å®š advantage è´¨é‡ï¼Œè¿›è€Œå†³å®š policy æ›´æ–°æ–¹å‘æ˜¯å¦æ­£ç¡®ã€‚

**æ€ä¹ˆè¯»**

- åˆæœŸè¾ƒé«˜ï¼Œéšåä¸‹é™æˆ–è¶‹ç¨³ï¼šæ­£å¸¸
- **æŒç»­ä¸Šå‡**ï¼šå¸¸è§åŸå› ï¼š
  1. å›æŠ¥åˆ†å¸ƒåœ¨å˜ï¼ˆreward shapingã€terminationã€AMP reward scale å˜åŒ–ï¼‰
  2. critic å­¦ä¹ ç‡/å®¹é‡ä¸å¤Ÿ
  3. policy å†»ç»“å¯¼è‡´é‡‡æ ·åˆ†å¸ƒå•ä¸€ï¼Œä½† returns å™ªå£°ä»åœ¨ï¼ˆcritic ä¼šâ€œå­¦ä¸å‡†â€ï¼‰
  4. å½’ä¸€åŒ–ï¼ˆobs/reward/advantageï¼‰ä¸ä¸€è‡´

ä½ ä¹‹å‰çš„å›¾é‡Œ value_function æœ‰ä¸Šå‡è¶‹åŠ¿ï¼Œè¿™é€šå¸¸æç¤ºï¼š

> ä¸æ˜¯æ•°å€¼çˆ†ç‚¸ï¼Œè€Œæ˜¯â€œcritic æ­£åœ¨è·Ÿä¸ä¸Š returns çš„ç»“æ„â€ï¼Œå¸¸ç”± reward å¤±è¡¡ + policy å†»ç»“å¼•å‘ã€‚







#### å¤©å·¥TensorBoard  Perfå‚æ•°

##### **Perf / collection_time**

**é‡‡æ ·æ—¶é—´ï¼ˆç¯å¢ƒäº¤äº’è€—æ—¶ï¼‰**

ğŸ“ˆ ä½ å›¾ä¸­è¡¨ç°ï¼š

- ç¨³å®šåœ¨ **â‰ˆ 0.41 s**
- æ³¢åŠ¨å¾ˆå°ï¼Œæ²¡æœ‰è¶‹åŠ¿æ€§å¢é•¿æˆ–ä¸‹é™



##### Perf / **learning_time**

**å•æ¬¡æ›´æ–°ï¼ˆåå‘ä¼ æ’­ï¼‰è€—æ—¶**

ğŸ“ˆ ä½ å›¾ä¸­ï¼š

- ç¨³å®šåœ¨ ~0.061s
- æ³¢åŠ¨æå°

è§£é‡Šï¼š

- ç½‘ç»œè§„æ¨¡ã€batch sizeã€optimizer éƒ½æ˜¯ç¨³å®šçš„
- GPU æ²¡æœ‰è¿‡è½½
- æ²¡æœ‰å‡ºç°æ¢¯åº¦çˆ†ç‚¸ / NaN

#####  **total_fps**

**æ•´ä½“è®­ç»ƒé€Ÿåº¦ï¼ˆenv step / secondï¼‰**

ğŸ“ˆ ä½ å›¾ä¸­ï¼š

- å¤§çº¦åœ¨ 3200â€“3300 fps æ³¢åŠ¨
- ç¨³å®šï¼Œæ²¡æœ‰æ˜æ˜¾ä¸‹é™

è§£é‡Šï¼š

- å¹¶è¡Œç¯å¢ƒæ•°åˆç†
- æ²¡æœ‰æ€§èƒ½ç“¶é¢ˆ



#### å¤©å·¥TensorBoard  Trainå‚æ•°

##### Train / mean_episode_length

ï¼ˆå¹³å‡ episode é•¿åº¦ï¼‰

ğŸ“ˆ èµ°åŠ¿ï¼š

- ä» ~65 æŒç»­ä¸Šå‡åˆ° ~81
- ç¨³å®šå¢é•¿ï¼Œæ²¡æœ‰å›è½

è§£è¯»ï¼š

- agent è¶Šæ¥è¶Šä¸å®¹æ˜“â€œå¤±è´¥â€
- episode æ›´é•¿

##### Train / mean_episode_length_time

è¿™æ˜¯ episode çš„**çœŸå®æ—¶é—´é•¿åº¦ï¼ˆç§’ï¼‰**ã€‚

ğŸ“ˆ ä» ~80s â†’ ~85s

è¯´æ˜ï¼š

- ä¸ step æ•°ä¸€è‡´å¢é•¿
- ä¸æ˜¯ simulation bug



##### Train / mean_reward

ğŸ“ˆ ä½ è¿™æ¡éå¸¸å…³é”®ï¼š

- ä»å¤§çº¦ **-1.2 â†’ -0.3**
- æŒç»­ä¸Šå‡ï¼Œä½†å§‹ç»ˆä¸ºè´Ÿ
- åæœŸä¸Šå‡æ˜æ˜¾å˜æ…¢

è§£é‡Šï¼š

- reward ç¡®å®åœ¨å˜å¥½ï¼ˆæ¨¡å‹åœ¨å­¦ï¼‰
- ä½†è¢« **è´Ÿé¡¹ï¼ˆæƒ©ç½šï¼‰ä¸»å¯¼**
- æ°¸è¿œâ€œèµšä¸åˆ°æ­£æ”¶ç›Šâ€

è¿™å’Œä½  reward é‡Œï¼š

- energy
- dof limits
- ankle torque
   ç­‰é¡¹è¿‡å¼ºå®Œå…¨ä¸€è‡´ã€‚



##### Train / mean_reward_time

è¿™æ¡æ˜¯ time-weighted rewardï¼ˆéšæ—¶é—´ç´¯è®¡ï¼‰

è¡¨ç°ï¼š

- è¶‹åŠ¿ç±»ä¼¼
- æŠ–åŠ¨æ›´æ˜æ˜¾ï¼ˆå› ä¸º reward æ³¢åŠ¨ï¼‰

è¯´æ˜ï¼š

- æ¨¡å‹åœ¨æŸäº›é˜¶æ®µå°è¯•â€œèµ°â€ï¼Œä½†å¾ˆå¿«è¢«æƒ©ç½šæ‹‰å›









# å¼ºåŒ–å­¦ä¹  

## ç­–ç•¥æ¢¯åº¦ç®—æ³•

ç»™å®šå‚æ•°åŒ–ç­–ç•¥  
\[
\pi_\theta(a \mid s)
\]

ç›®æ ‡æ˜¯æœ€å¤§åŒ–æœŸæœ›æŠ˜æ‰£å›æŠ¥ï¼š
\[
J(\theta)
=
\mathbb{E}_{\tau \sim \pi_\theta}
\left[
\sum_{t=0}^{T-1} \gamma^t r_t
\right]
\]

å…¶ä¸­è½¨è¿¹  
\[
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots)
\]
ç”±ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’é‡‡æ ·å¾—åˆ°ã€‚



### ç­–ç•¥æ¢¯åº¦å®šç†ï¼ˆREINFORCE æ ¸å¿ƒï¼‰

ä½¿ç”¨ **log-derivative trick**ï¼š

\[
\nabla_\theta J(\theta)
=
\mathbb{E}
\left[
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t \mid s_t)
\;
G_t
\right]
\]

å…¶ä¸­è’™ç‰¹å¡æ´›å›æŠ¥ï¼š
\[
G_t
=
\sum_{k=t}^{T-1} \gamma^{k-t} r_k
\]

å› æ­¤å¯æ„é€ æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼š
\[
\mathcal{L}(\theta)
=
- \mathbb{E}
\big[
\log \pi_\theta(a_t \mid s_t)\; G_t
\big]
\]

å¯¹è¯¥ loss åšæ¢¯åº¦ä¸‹é™  
â‡”  
å¯¹ \( J(\theta) \) åšæ¢¯åº¦ä¸Šå‡ã€‚



ä¼˜åŒ–å¯¹è±¡ï¼š
$$
\theta \leftarrow \theta + \alpha \frac{1}{T}\sum_{t=0}^{T-1} A_t \nabla_\theta \log\pi_\theta(a_t|s_t)
$$

- æ³¨æ„è¿™é‡Œæ˜¯ä¸€ä¸ª **ä¹˜ç§¯**ï¼š

  - $$
    \nabla_\theta \log \pi_\theta(a_t \mid s_t)
    $$
    
     
    ğŸ‘‰ æ˜¯ä¸€ä¸ª **å‘é‡**ï¼ˆç»´åº¦ = å‚æ•°ä¸ªæ•°ï¼‰
    
  - $$
    A_t
    $$
    
    ğŸ‘‰ æ˜¯ä¸€ä¸ª **æ ‡é‡æƒé‡**


$$

$$

## A2Cç®—æ³•









## PPOç®—æ³•

##### GAE

**GAEï¼ˆGeneralized Advantage Estimationï¼Œå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰**
 æ˜¯ä¸€ç§ **ç”¨ TD æ®‹å·®çš„æŒ‡æ•°åŠ æƒå’Œæ¥ä¼°è®¡ä¼˜åŠ¿å‡½æ•° A(s_t,a_t)** çš„æ–¹æ³•ï¼Œç”¨æ¥åœ¨**é«˜æ–¹å·®ï¼ˆMonte Carloï¼‰**å’Œ**é«˜åå·®ï¼ˆTDï¼‰**ä¹‹é—´åšè¿ç»­å¯è°ƒçš„æŠ˜ä¸­ã€‚

###  Vã€Qã€Aã€r çš„å«ä¹‰ä¸å…³ç³»

åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆå°¤å…¶æ˜¯ PPO / Actorâ€“Critic æ–¹æ³•ï¼‰ä¸­ï¼Œ**rã€Vã€Qã€A** æ˜¯æœ€æ ¸å¿ƒã€ä½†ä¹Ÿæœ€å®¹æ˜“æ··æ·†çš„å››ä¸ªæ¦‚å¿µã€‚ä¸‹é¢ä»**å®šä¹‰ â†’ æ•°å­¦å½¢å¼ â†’ ç›´è§‰ç†è§£ â†’ ä»£ç å¯¹åº”**å››ä¸ªå±‚é¢ç»Ÿä¸€è¯´æ˜ã€‚

------

####  r 

**â€”â€” Rewardï¼ˆå³æ—¶å¥–åŠ±ï¼‰**

**æ•°å­¦å®šä¹‰**
$$
r_t = r(s_t, a_t, s_{t+1})
$$
**å«ä¹‰**

> **ç¯å¢ƒåœ¨å½“å‰æ—¶é—´æ­¥ç»™å‡ºçš„å³æ—¶åé¦ˆ**

- æ¥è‡ªç¯å¢ƒï¼Œè€Œéæ¨¡å‹å­¦ä¹ 
- åªåæ˜ â€œå½“å‰è¿™ä¸€æ­¥â€çš„å¥½å
- ä¸åŒ…å«å¯¹æœªæ¥çš„ç›´æ¥åˆ¤æ–­

**ç›´è§‰ç†è§£**

> â€œæˆ‘åˆšåˆšè¿™ä¸€æ­¥ï¼Œç¯å¢ƒç»™æˆ‘æ‰“äº†å¤šå°‘åˆ†ï¼Ÿâ€

CartPole ä¸­

- æ¯ä¸€æ­¥æœªå€’ï¼š`r = 1`
- æ†å€’æˆ–è¶Šç•Œï¼šepisode ç»“æŸ

PPO ä»£ç ä¸­çš„ä½ç½®

```
next_obs, reward, terminated, truncated, _ = env.step(action)
```

è¿™é‡Œçš„ `reward` å³ $r_t$ã€‚

------

#### V 

**â€”â€” State Value Functionï¼ˆçŠ¶æ€ä»·å€¼å‡½æ•°ï¼‰**

**æ•°å­¦å®šä¹‰**
$$
V^\pi(s) = \mathbb{E}_\pi\!\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k} \;\middle|\; s_t = s\right]
$$
**å«ä¹‰**

> **ç«™åœ¨çŠ¶æ€ sä¸Šï¼Œåœ¨å½“å‰ç­–ç•¥ piä¸‹ï¼Œæœªæ¥è¿˜èƒ½æœŸæœ›è·å¾—å¤šå°‘ç´¯è®¡å›æŠ¥**

- åªä¸çŠ¶æ€æœ‰å…³ï¼Œä¸æŒ‡å®šåŠ¨ä½œ
- æ˜¯ä¸€ä¸ªæœŸæœ›å€¼ï¼ˆå¹³å‡æ„ä¹‰ï¼‰
- ç”± Critic ç½‘ç»œè¿‘ä¼¼å­¦ä¹ 

**ç›´è§‰ç†è§£**

> â€œè¿™ä¸ªä½ç½®æ•´ä½“ä¸Šå€¼ä¸å€¼ï¼Ÿâ€

PPO **ä»£ç ä¸­çš„ä½ç½®**

```
logits, value = model(obs)
```

å…¶ä¸­ `value` å³ V(s_t)

------

#### Q 

**â€”â€” Action-Value Functionï¼ˆåŠ¨ä½œä»·å€¼å‡½æ•°ï¼‰**

**æ•°å­¦å®šä¹‰**
$$
Q^\pi(s,a) = \mathbb{E}_\pi\!\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k} 
\;\middle|\; s_t = s,\; a_t = a\right]
$$
**å«ä¹‰**

> **åœ¨çŠ¶æ€ $s$ ä¸‹ï¼Œå¦‚æœå½“å‰è¿™ä¸€æ­¥é€‰æ‹©åŠ¨ä½œ $a$ï¼Œæœªæ¥æœŸæœ›èƒ½è·å¾—å¤šå°‘ç´¯è®¡å›æŠ¥**

- åŒæ—¶ä¾èµ–çŠ¶æ€å’ŒåŠ¨ä½œ
- ç²’åº¦æ¯” V æ›´ç»†
- åœ¨ DQN ç­‰æ–¹æ³•ä¸­ä¼šè¢«æ˜¾å¼å»ºæ¨¡

**ç›´è§‰ç†è§£**

> â€œåœ¨è¿™ä¸ªä½ç½®ï¼Œèµ°è¿™ä¸€æ­¥å€¼ä¸å€¼ï¼Ÿâ€

**åœ¨ PPO ä¸­å¦‚ä½•å¾—åˆ° Qï¼Ÿ**

PPO **ä¸æ˜¾å¼å­¦ä¹  Q ç½‘ç»œ**ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€æ­¥ bootstrap è¿‘ä¼¼ï¼š
$$
Q(s_t,a_t) \;\approx\; r_t + \gamma(1-d_t)V(s_{t+1})
$$




#### è¯æ˜

**æŠŠâ€œæœªæ¥å›æŠ¥â€æ‹†æˆâ€œç¬¬ä¸€æ­¥ + å‰©ä½™éƒ¨åˆ†â€**

å¯¹æ— ç©·å’Œåšä»£æ•°æ‹†åˆ†ï¼š
$$
\sum_{k=0}^{\infty}\gamma^k r_{t+k}
=
r_t
+
\gamma \sum_{k=0}^{\infty}\gamma^k r_{t+1+k}
$$
è¿™æ˜¯**çº¯ä»£æ•°æ’ç­‰å¼**ã€‚

ä»£å› Q çš„å®šä¹‰ï¼š
$$
Q^\pi(s_t,a_t)
=
\mathbb{E}_\pi
\!\left[
r_t
+
\gamma \sum_{k=0}^{\infty}\gamma^k r_{t+1+k}
\;\middle|\;
s_t,a_t
\right]
$$
ç°åœ¨å…³æ³¨ååŠé¡¹ï¼š

é€šè¿‡å…¬å¼å¯ä»¥è¯æ˜
$$
\mathbb{E}_\pi
\!\left[
\sum_{k=0}^{\infty}\gamma^k r_{t+1+k}
\;\middle|\;
s_{t+1}
\right]
=
V^\pi(s_{t+1})
$$


**ç†è®ºå±‚é¢ï¼ˆä¸¥æ ¼ï¼‰**
$$
Q^\pi(s,a)
=
\mathbb{E}\big[
r + \gamma V^\pi(s')
\big]
\qquad \text{ï¼ˆä¸¥æ ¼æˆç«‹ï¼‰}
$$

------

**ç®—æ³•å±‚é¢ï¼ˆPPO / Actorâ€“Criticï¼‰**

åœ¨å®é™…ç®—æ³•ä¸­ï¼š

- çœŸå®çš„ $V^\pi$ ä¸å¯å¾—
- åªèƒ½ä½¿ç”¨ä¸€ä¸ªå­¦ä¹ åˆ°çš„ Critic ç½‘ç»œï¼š

$$
V_\phi(s) \approx V^\pi(s)
$$

äºæ˜¯ï¼Œåœ¨**æ ·æœ¬å±‚é¢**ä½¿ç”¨ï¼š
$$
Q(s_t,a_t)
\;\approx\;
r_t + \gamma V_\phi(s_{t+1})
$$










------

#### A 

â€”â€” **Advantage** Functionï¼ˆä¼˜åŠ¿å‡½æ•°ï¼‰

**æ•°å­¦å®šä¹‰**
$$
A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
$$
**å«ä¹‰ï¼ˆPPO çš„æ ¸å¿ƒï¼‰**

> **åœ¨å½“å‰çŠ¶æ€ä¸‹ï¼Œé€‰è¿™ä¸ªåŠ¨ä½œæ¯”â€œå¹³å‡åŠ¨ä½œâ€å¥½å¤šå°‘**

- æ˜¯ä¸€ä¸ªç›¸å¯¹é‡
- $A > 0$ï¼šå¥½äºå¹³å‡ï¼Œåº”è¯¥æé«˜æ¦‚ç‡
- $A < 0$ï¼šå·®äºå¹³å‡ï¼Œåº”è¯¥é™ä½æ¦‚ç‡

**ä¸ºä»€ä¹ˆç­–ç•¥æ¢¯åº¦ç”¨** Aï¼Ÿ

- ç›´æ¥ç”¨ Q â†’ æ–¹å·®å¤§
- å‡å» V ä½œä¸º baseline â†’ **ä¸æ”¹å˜æœŸæœ›æ¢¯åº¦ï¼Œä½†æ˜¾è‘—é™ä½æ–¹å·®**

**PPO ä»£ç ä¸­çš„ä½ç½®**

ä¼˜åŠ¿ç”± TD æ®‹å·® + GAE è®¡ç®—ï¼š

```
buffer.compute_gae(...)
adv_b = buffer.advantages
```

------

#### å››è€…ä¹‹é—´çš„å› æœå…³ç³»

```
ç¯å¢ƒåé¦ˆï¼š        r_t
                   â†“
Critic ä¼°è®¡ï¼š   V(s_t), V(s_{t+1})
                   â†“
ä¸€æ­¥ Q è¿‘ä¼¼ï¼š   Q(s_t,a_t) â‰ˆ r_t + Î³ V(s_{t+1})
                   â†“
ä¼˜åŠ¿å®šä¹‰ï¼š     A(s_t,a_t) = Q - V(s_t)
```

------

#### åœ¨ PPO ä¸­å„è‡ªçš„è§’è‰²åˆ†å·¥

| é‡   | æ¥æº        | ä½œç”¨                 |
| ---- | ----------- | -------------------- |
| r    | ç¯å¢ƒ        | æä¾›çœŸå®å³æ—¶åé¦ˆ     |
| V    | Critic ç½‘ç»œ | baseline + bootstrap |
| Q    | éšå¼è¿‘ä¼¼    | æ„é€ ä¼˜åŠ¿             |
| A    | GAE è®¡ç®—    | Actor æ›´æ–°æ–¹å‘       |

------

#### ä¸€å¥è¯ç»ˆæè®°å¿†æ³•

> **r æ˜¯å½“ä¸‹å‘ç”Ÿçš„äº‹ï¼Œ
>  V æ˜¯ç«™åœ¨è¿™é‡Œçš„æ€»ä½“æœŸæœ›ï¼Œ
>  Q æ˜¯èµ°è¿™ä¸€æ­¥åçš„é•¿æœŸä»·å€¼ï¼Œ
>  A æ˜¯â€œè¿™ä¸€æ­¥æ¯”å¹³å‡å¥½å¤šå°‘â€ã€‚**







# è¶…å£°å¯¼èˆª

## æ–‡ç« æ€»æ½

**æ¸…åå¤§å­¦ Ã— åŒ—äº¬æ™ºæº**

æ¸…å Cardiac Copilot: Automatic Probe Guidance for Echocardiography with World Model    [Jiang, Haojun](https://papers.miccai.org/miccai-2024/tags#Jiang, Haojun)

æ¸…å UltraSeP: Sequence-aware pre-training for echocardiography probe movement guidance å’Œä¸Šé¢åŒè¯¾é¢˜ç»„

æ¸…å EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe Guidance åŒè¯¾é¢˜ç»„

VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance





**è‹±å›½ Oxford / Kingâ€™s College London** 

Droste et al., Automatic probe movement guidance for freehand obstetric ultrasoundï¼ˆMICCAI 2020ï¼‰



**ç¾å›½ Ã— å•†ä¸šåŒ»ç–— AIï¼ˆNarang / JAMA Cardiologyï¼‰**

Utility of a deep-learning algorithm to guide novices to acquire echocardiogramsï¼ˆJAMA Cardiology, 2021ï¼‰



**æ—¥æœ¬ä¸œäº¬å¤§å­¦ / å²©ç”°å¼˜ä¸€ï¼ˆIwata Labï¼‰ æœºå™¨äººå¿ƒè„è¶…å£°**

Automated image acquisition of PLAX with robotic echocardiographyï¼ˆRA-L 2023ï¼‰

Diagnostic posture control system for seated-style echocardiography robotï¼ˆIJCARS 2023ï¼‰





[Guide2Heart: Proximity Guidance for Standard Echocardiographic View](https://link.springer.com/chapter/10.1007/978-3-032-06329-8_18)  2025å¹´9æœˆ å°åº¦ç†å·¥å­¦é™¢ å‘è¡¨åœ¨ Simplifying Medical Ultrasound

[Real-Time Echocardiography Guidance for Optimized Apical Standard Views](https://www.sciencedirect.com/science/article/pii/S0301562922005658)   2023å¹´1æœˆ æŒªå¨ç§‘æŠ€å¤§å­¦  David Pasdeloup 









































